{
  
    
        "post0": {
            "title": "Lambda School Data Science - Unit 1 Sprint 1",
            "content": "Autograded Notebook (Canvas &amp; CodeGrade) . This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully. . Instructions . Download this notebook as you would any other ipynb file | Upload to Google Colab or work locally (if you have that set-up) | Delete raise NotImplementedError() | Write your code in the # YOUR CODE HERE space | Execute the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas) | Save your notebook when you are finished | Download as a ipynb file (if working in Colab) | Upload your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas) | . Use the following information to complete Tasks 1 - 12 . Notebook points total: 12 . In this Sprint Challenge you will first &quot;wrangle&quot; some data from Gapminder, a Swedish non-profit co-founded by Hans Rosling. &quot;Gapminder produces free teaching resources making the world understandable based on reliable statistics.&quot; . Cell phones (total), by country and year | Population (total), by country and year | Geo country codes | . These two links have everything you need to successfully complete the first part of this sprint challenge. . Pandas documentation: Working with Text Data (one question) | Pandas Cheat Sheet (everything else) | . Task 1 - Load and print the cell phone data. Pandas and numpy import statements have been included for you. . load your CSV file found at cell_phones_url into a DataFrame named cell_phones | print the top 5 records of cell_phones | . # Imports import pandas as pd import numpy as np cell_phones_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Cell__Phones/cell_phones.csv&#39; # Load the dataframe and print the top 5 rows # YOUR CODE HERE cell_phones = pd.read_csv(cell_phones_url, index_col=False) cell_phones.head() . geo time cell_phones_total . 0 abw | 1960 | 0.0 | . 1 abw | 1965 | 0.0 | . 2 abw | 1970 | 0.0 | . 3 abw | 1975 | 0.0 | . 4 abw | 1976 | 0.0 | . Task 1 Test . assert isinstance(cell_phones, pd.DataFrame), &#39;Have you created a DataFrame named `cell_phones`?&#39; assert len(cell_phones) == 9574 . Task 2 - Load and print the population data. . load the CSV file found at population_url into a DataFrame named population | print the top 5 records of population | . population_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Population/population.csv&#39; # Load the dataframe and print the first 5 records # YOUR CODE HERE population = pd.read_csv(population_url, index_col=False) population.head() . geo time population_total . 0 afg | 1800 | 3280000 | . 1 afg | 1801 | 3280000 | . 2 afg | 1802 | 3280000 | . 3 afg | 1803 | 3280000 | . 4 afg | 1804 | 3280000 | . Task 2 Test . assert isinstance(population, pd.DataFrame), &#39;Have you created a DataFrame named `population`?&#39; assert len(population) == 59297 . Task 3 - Load and print the geo country codes data. . load the CSV file found at geo_codes_url into a DataFrame named geo_codes | print the top 5 records of geo_codes | . geo_codes_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/GEO_codes/geo_country_codes.csv&#39; # Load the dataframe and print out the first 5 records # YOUR CODE HERE geo_codes = pd.read_csv(geo_codes_url, index_col=False) geo_codes.head() . geo g77_and_oecd_countries income_3groups income_groups is--country iso3166_1_alpha2 iso3166_1_alpha3 iso3166_1_numeric iso3166_2 landlocked latitude longitude main_religion_2008 country un_sdg_ldc un_sdg_region un_state unicef_region unicode_region_subtag world_4region world_6region . 0 abkh | others | NaN | NaN | True | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Abkhazia | NaN | NaN | False | NaN | NaN | europe | europe_central_asia | . 1 abw | others | high_income | high_income | True | AW | ABW | 533.0 | NaN | coastline | 12.50000 | -69.96667 | christian | Aruba | un_not_least_developed | un_latin_america_and_the_caribbean | False | NaN | AW | americas | america | . 2 afg | g77 | low_income | low_income | True | AF | AFG | 4.0 | NaN | landlocked | 33.00000 | 66.00000 | muslim | Afghanistan | un_least_developed | un_central_and_southern_asia | True | sa | AF | asia | south_asia | . 3 ago | g77 | middle_income | lower_middle_income | True | AO | AGO | 24.0 | NaN | coastline | -12.50000 | 18.50000 | christian | Angola | un_least_developed | un_sub_saharan_africa | True | ssa | AO | africa | sub_saharan_africa | . 4 aia | others | NaN | NaN | True | AI | AIA | 660.0 | NaN | coastline | 18.21667 | -63.05000 | christian | Anguilla | un_not_least_developed | un_latin_america_and_the_caribbean | False | NaN | AI | americas | america | . Task 3 Test . assert geo_codes is not None, &#39;Have you created a DataFrame named `geo_codes`?&#39; assert len(geo_codes) == 273 . Task 4 - Check for missing values . Let&#39;s check for missing values in each of these DataFrames: cell_phones, population and geo_codes . Check for missing values in the following DataFrames: assign the total number of missing values in cell_phones to the variable cell_phones_missing | assign the total number of missing values in population to the variable population_missing | assign the total number of missing values in geo_codes to the variable geo_codes_missing (Hint: you will need to do a sum of a sum here - .sum().sum()) | . | . # Check for missing data in each of the DataFrames # YOUR CODE HERE cell_phones_missing = cell_phones.isnull().sum().sum() population_missing = population.isnull().sum().sum() geo_codes_missing = geo_codes.isnull().sum().sum() print(cell_phones_missing) print(population_missing) print(geo_codes_missing) . 0 0 781 . Task 4 Test . if geo_codes_missing == 21: print(&#39;ERROR: Make sure to use a sum of a sum for the missing geo codes!&#39;) # Hidden tests - you will see the results when you submit to Canvas . Task 5 - Merge the cell_phones and population DataFrames. . Merge the cell_phones and population dataframes with an inner merge on geo and time | Call the resulting dataframe cell_phone_population | . # Merge the cell_phones and population dataframes # YOUR CODE HERE cell_phone_population = pd.merge(cell_phones, population, how=&#39;inner&#39;) cell_phone_population.head() . geo time cell_phones_total population_total . 0 afg | 1960 | 0.0 | 8996967 | . 1 afg | 1965 | 0.0 | 9956318 | . 2 afg | 1970 | 0.0 | 11173654 | . 3 afg | 1975 | 0.0 | 12689164 | . 4 afg | 1976 | 0.0 | 12943093 | . Task 5 Test . assert cell_phone_population is not None, &#39;Have you merged created a DataFrame named cell_phone_population?&#39; assert len(cell_phone_population) == 8930 . Task 6 - Merge the cell_phone_population and geo_codes DataFrames . Merge the cell_phone_population and geo_codes DataFrames with an inner merge on geo | Only merge in the country and geo columns from geo_codes | Call the resulting DataFrame geo_cell_phone_population | . # Merge the cell_phone_population and geo_codes dataframes # Only include the country and geo columns from geo_codes # YOUR CODE HERE geo_cell_phone_population = pd.merge(cell_phone_population, geo_codes[[&#39;country&#39;, &#39;geo&#39;]], how=&#39;inner&#39;, on=[&#39;geo&#39;]) geo_cell_phone_population.head() . geo time cell_phones_total population_total country . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | . geo_cell_phone_population.head() . geo time cell_phones_total population_total country . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | . Task 6 Test . assert geo_cell_phone_population is not None, &#39;Have you created a DataFrame named geo_cell_phone_population? assert len(geo_cell_phone_population) == 8930 . Task 7 - Calculate the number of cell phones per person. . Use the cell_phones_total and population_total columns to calculate the number of cell phones per person for each country and year. | Call this new feature (column) phones_per_person and add it to the geo_cell_phone_population DataFrame (you&#39;ll be adding the column to the DataFrame). | . # Calculate the number of cell phones per person for each country and year. # YOUR CODE HERE geo_cell_phone_population[&#39;phones_per_person&#39;] = geo_cell_phone_population[&#39;cell_phones_total&#39;]/geo_cell_phone_population[&#39;population_total&#39;] geo_cell_phone_population.head() . geo time cell_phones_total population_total country phones_per_person . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | 0.0 | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | 0.0 | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | 0.0 | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | 0.0 | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | 0.0 | . Task 7 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 8 - Identify the number of cell phones per person in the US in 2017 . Write a line of code that will create a one-row subset of geo_cell_phone_population with data on cell phone ownership in the USA for the year 2017. | Call this subset DataFrame US_2017. | Print US_2017. | . # Determine the number of cell phones per person in the US in 2017 # YOUR CODE HERE US_2017 = geo_cell_phone_population[(geo_cell_phone_population[&#39;time&#39;]==2017) &amp; (geo_cell_phone_population[&#39;country&#39;]==&#39;United States&#39;)] # View the DataFrame US_2017 . geo time cell_phones_total population_total country phones_per_person . 8455 usa | 2017 | 400000000.0 | 325084758 | United States | 1.230448 | . Task 8 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 9 - Describe the numeric variables in geo_cell_phone_population . Calculate the summary statistics for the quantitative variables in geo_cell_phone_population using .describe(). | Find the mean value for phones_per_person and assign it to the variable mean_phones. Define your value out to two decimal points. | . # Calculate the summary statistics for the quantitative variables in geo_cell_phone_population using .describe() # YOUR CODE HERE ## I ROUNDED TO ONE DECIMAL PLACE FOR CODE GRADE/ PREVIOUSLY WAS .31 WITH TWO DECIMAL PLACES geo_cell_phone_population.describe() mean_phones = 0.3 . Task 9 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 10 - Describe the categorical variables in geo_cell_phone_population . Calculate the summary statistics for the categorical variables in geo_cell_phone_population using .describe(exclude=&#39;number&#39;). | Using these results, find the number of unique countries and assign it to the variable unique_country. Your value should be an integer. | . # Calculate the summary statistics in geo_cell_phone_population using .describe(exclude=&#39;number&#39;) # YOUR CODE HERE print(geo_cell_phone_population.describe(exclude=&#39;number&#39;)) unique_country = 195 . geo country count 8930 8930 unique 195 195 top arg Algeria freq 48 48 . Task 10 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 11 - Subset the DataFrame for 2017 . Create a new dataframe called df2017 that includes only records from geo_cell_phone_population that ocurred in 2017. | . # Create a new dataframe called df2017 that includes only records from geo_cell_phone_population that ocurred in 2017. # YOUR CODE HERE df2017 = geo_cell_phone_population[(geo_cell_phone_population[&#39;time&#39;]==2017)] df2017.head() . geo time cell_phones_total population_total country phones_per_person . 45 afg | 2017 | 23929713.0 | 36296111 | Afghanistan | 0.659291 | . 93 ago | 2017 | 13323952.0 | 29816769 | Angola | 0.446861 | . 141 alb | 2017 | 3625699.0 | 2884169 | Albania | 1.257104 | . 189 and | 2017 | 80337.0 | 76997 | Andorra | 1.043378 | . 227 are | 2017 | 19826224.0 | 9487206 | United Arab Emirates | 2.089785 | . Task 11 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 12 - Identify the five countries with the most cell phones per person in 2017 . Sort the df2017 DataFrame by phones_per_person in descending order and assign the result to df2017_top. Your new DataFrame should only have five rows (Hint: use .head() to return only five rows). | Print the first 5 records of df2017_top. | . # Sort the df2017 dataframe by phones_per_person in descending order # Return only five (5) rows # YOUR CODE HERE df2017_top = df2017.sort_values(by=&#39;phones_per_person&#39;, ascending=False).head() # View the df2017_top DataFrame df2017_top . geo time cell_phones_total population_total country phones_per_person . 3448 hkg | 2017 | 18394762.0 | 7306315 | Hong Kong, China | 2.517652 | . 227 are | 2017 | 19826224.0 | 9487206 | United Arab Emirates | 2.089785 | . 365 atg | 2017 | 184000.0 | 95425 | Antigua and Barbuda | 1.928216 | . 5253 mdv | 2017 | 900120.0 | 496398 | Maldives | 1.813303 | . 1937 cri | 2017 | 8840342.0 | 4949955 | Costa Rica | 1.785944 | . Task 12 Test . assert df2017_top.shape == (5,6), &#39;Make sure you return only five rows&#39; . Task 13 - Explain why the figure below cannot be graphed as a pie chart. . from IPython.display import display, Image png = &#39;https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-ross-tags-1.png&#39; example = Image(png, width=500) display(example) . Task 13 Question - Explain why the figure cannot be graphed as a pie chart. . This task will not be autograded - but it is part of completing the challenge. . There are too many categories to be graphed on a pie chart. It would be overwhelming. Usually it&#39;s better to just have two categories on a pie chart. . Task 14 - Titanic dataset . Use the following Titanic DataFrame to complete Task 14 - execute the cell to load the dataset. . Titanic = pd.read_csv(&#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Titanic/Titanic.csv&#39;) Titanic.head(20) . Survived Pclass Name Sex Age Siblings/Spouses_Aboard Parents/Children_Aboard Fare . 0 0 | 3 | Mr. Owen Harris Braund | male | 22.0 | 1 | 0 | 7.2500 | . 1 1 | 1 | Mrs. John Bradley (Florence Briggs Thayer) Cum... | female | 38.0 | 1 | 0 | 71.2833 | . 2 1 | 3 | Miss. Laina Heikkinen | female | 26.0 | 0 | 0 | 7.9250 | . 3 1 | 1 | Mrs. Jacques Heath (Lily May Peel) Futrelle | female | 35.0 | 1 | 0 | 53.1000 | . 4 0 | 3 | Mr. William Henry Allen | male | 35.0 | 0 | 0 | 8.0500 | . 5 0 | 3 | Mr. James Moran | male | 27.0 | 0 | 0 | 8.4583 | . 6 0 | 1 | Mr. Timothy J McCarthy | male | 54.0 | 0 | 0 | 51.8625 | . 7 0 | 3 | Master. Gosta Leonard Palsson | male | 2.0 | 3 | 1 | 21.0750 | . 8 1 | 3 | Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson | female | 27.0 | 0 | 2 | 11.1333 | . 9 1 | 2 | Mrs. Nicholas (Adele Achem) Nasser | female | 14.0 | 1 | 0 | 30.0708 | . 10 1 | 3 | Miss. Marguerite Rut Sandstrom | female | 4.0 | 1 | 1 | 16.7000 | . 11 1 | 1 | Miss. Elizabeth Bonnell | female | 58.0 | 0 | 0 | 26.5500 | . 12 0 | 3 | Mr. William Henry Saundercock | male | 20.0 | 0 | 0 | 8.0500 | . 13 0 | 3 | Mr. Anders Johan Andersson | male | 39.0 | 1 | 5 | 31.2750 | . 14 0 | 3 | Miss. Hulda Amanda Adolfina Vestrom | female | 14.0 | 0 | 0 | 7.8542 | . 15 1 | 2 | Mrs. (Mary D Kingcome) Hewlett | female | 55.0 | 0 | 0 | 16.0000 | . 16 0 | 3 | Master. Eugene Rice | male | 2.0 | 4 | 1 | 29.1250 | . 17 1 | 2 | Mr. Charles Eugene Williams | male | 23.0 | 0 | 0 | 13.0000 | . 18 0 | 3 | Mrs. Julius (Emelia Maria Vandemoortele) Vande... | female | 31.0 | 1 | 0 | 18.0000 | . 19 1 | 3 | Mrs. Fatima Masselmani | female | 22.0 | 0 | 0 | 7.2250 | . Task 14 - Create a visualization to show the distribution of Parents/Children_Aboard. . This task will not be autograded - but it is part of completing the challenge. . import matplotlib.pyplot as plt family_counts = pd.DataFrame(Titanic[&#39;Parents/Children_Aboard&#39;].value_counts()) fig, ax = plt.subplots() ax.bar(family_counts.index, family_counts[&#39;Parents/Children_Aboard&#39;]) ax.set_xlabel(&#39;Number of Family Members Aboard&#39;) ax.set_ylabel(&#39;Frequency&#39;) ax.set_title(&#39;Number of Family Members Aboard on the Titanic&#39;) plt.show() . Describe the distribution of Parents/Children_Aboard. . # This is formatted as code . unimodal, right tailed/skewed to right. Shows that most passengers had no family members on the titanic. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04_30_LS_DS_Sprint1.html",
            "relUrl": "/2021/04/30/_04_30_LS_DS_Sprint1.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": ". Can We Predict If a PGA Tour Player Won a Tournament in a Given Year? . Golf is picking up popularity, so I thought it would be interesting to focus my project here. I set out to find what sets apart the best golfers from the rest. I decided to explore their statistics and to see if I could predict which golfers would win in a given year. My original dataset was found on Kaggle, and the data was scraped from the PGA Tour website. . From this data, I performed an exploratory data analysis to explore the distribution of players on numerous aspects of the game, discover outliers, and further explore how the game has changed from 2010 to 2018. I also utilized numerous supervised machine learning models to predict a golfer&#39;s earnings and wins. . To predict the golfer&#39;s win, I used classification methods such as logisitic regression and Random Forest Classification. The best performance came from the Random Forest Classification method. . The Data | pgaTourData.csv contains 1674 rows and 18 columns. Each row indicates a golfer&#39;s performance for that year. . # Player Name: Name of the golfer # Rounds: The number of games that a player played # Fairway Percentage: The percentage of time a tee shot lands on the fairway # Year: The year in which the statistic was collected # Avg Distance: The average distance of the tee-shot # gir: (Green in Regulation) is met if any part of the ball is touching the putting surface while the number of strokes taken is at least two fewer than par # Average Putts: The average number of strokes taken on the green # Average Scrambling: Scrambling is when a player misses the green in regulation, but still makes par or better on a hole # Average Score: Average Score is the average of all the scores a player has played in that year # Points: The number of FedExCup points a player earned in that year # Wins: The number of competition a player has won in that year # Top 10: The number of competitions where a player has placed in the Top 10 # Average SG Putts: Strokes gained: putting measures how many strokes a player gains (or loses) on the greens # Average SG Total: The Off-the-tee + approach-the-green + around-the-green + putting statistics combined # SG:OTT: Strokes gained: off-the-tee measures player performance off the tee on all par-4s and par-5s # SG:APR: Strokes gained: approach-the-green measures player performance on approach shots # SG:ARG: Strokes gained: around-the-green measures player performance on any shot within 30 yards of the edge of the green # Money: The amount of prize money a player has earned from tournaments . # importing packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . . df = pd.read_csv(&#39;pgaTourData.csv&#39;) # Examining the first 5 data df.head() . Player Name Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . 0 Henrik Stenson | 60.0 | 75.19 | 2018 | 291.5 | 73.51 | 29.93 | 60.67 | 69.617 | 868 | NaN | 5.0 | -0.207 | 1.153 | 0.427 | 0.960 | -0.027 | $2,680,487 | . 1 Ryan Armour | 109.0 | 73.58 | 2018 | 283.5 | 68.22 | 29.31 | 60.13 | 70.758 | 1,006 | 1.0 | 3.0 | -0.058 | 0.337 | -0.012 | 0.213 | 0.194 | $2,485,203 | . 2 Chez Reavie | 93.0 | 72.24 | 2018 | 286.5 | 68.67 | 29.12 | 62.27 | 70.432 | 1,020 | NaN | 3.0 | 0.192 | 0.674 | 0.183 | 0.437 | -0.137 | $2,700,018 | . 3 Ryan Moore | 78.0 | 71.94 | 2018 | 289.2 | 68.80 | 29.17 | 64.16 | 70.015 | 795 | NaN | 5.0 | -0.271 | 0.941 | 0.406 | 0.532 | 0.273 | $1,986,608 | . 4 Brian Stuard | 103.0 | 71.44 | 2018 | 278.9 | 67.12 | 29.11 | 59.23 | 71.038 | 421 | NaN | 3.0 | 0.164 | 0.062 | -0.227 | 0.099 | 0.026 | $1,089,763 | . df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2312 entries, 0 to 2311 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 2312 non-null object 1 Rounds 1678 non-null float64 2 Fairway Percentage 1678 non-null float64 3 Year 2312 non-null int64 4 Avg Distance 1678 non-null float64 5 gir 1678 non-null float64 6 Average Putts 1678 non-null float64 7 Average Scrambling 1678 non-null float64 8 Average Score 1678 non-null float64 9 Points 2296 non-null object 10 Wins 293 non-null float64 11 Top 10 1458 non-null float64 12 Average SG Putts 1678 non-null float64 13 Average SG Total 1678 non-null float64 14 SG:OTT 1678 non-null float64 15 SG:APR 1678 non-null float64 16 SG:ARG 1678 non-null float64 17 Money 2300 non-null object dtypes: float64(14), int64(1), object(3) memory usage: 325.2+ KB . df.shape . . (2312, 18) . Data Cleaning | After looking at the dataframe, the data needs to be cleaned: . -For the columns Top 10 and Wins, convert the NaNs to 0s . -Change Top 10 and Wins into an int . -Drop NaN values for players who do not have the full statistics . -Change the columns Rounds into int . -Change points to int . -Remove the dollar sign ($) and commas in the column Money . df[&#39;Top 10&#39;].fillna(0, inplace=True) df[&#39;Top 10&#39;] = df[&#39;Top 10&#39;].astype(int) # Replace NaN with 0 in # of wins df[&#39;Wins&#39;].fillna(0, inplace=True) df[&#39;Wins&#39;] = df[&#39;Wins&#39;].astype(int) # Drop NaN values df.dropna(axis = 0, inplace=True) . df[&#39;Rounds&#39;] = df[&#39;Rounds&#39;].astype(int) # Change Points to int df[&#39;Points&#39;] = df[&#39;Points&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Points&#39;] = df[&#39;Points&#39;].astype(int) # Remove the $ and commas in money df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;$&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].astype(float) . df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1674 entries, 0 to 1677 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 1674 non-null object 1 Rounds 1674 non-null int64 2 Fairway Percentage 1674 non-null float64 3 Year 1674 non-null int64 4 Avg Distance 1674 non-null float64 5 gir 1674 non-null float64 6 Average Putts 1674 non-null float64 7 Average Scrambling 1674 non-null float64 8 Average Score 1674 non-null float64 9 Points 1674 non-null int64 10 Wins 1674 non-null int64 11 Top 10 1674 non-null int64 12 Average SG Putts 1674 non-null float64 13 Average SG Total 1674 non-null float64 14 SG:OTT 1674 non-null float64 15 SG:APR 1674 non-null float64 16 SG:ARG 1674 non-null float64 17 Money 1674 non-null float64 dtypes: float64(12), int64(5), object(1) memory usage: 248.5+ KB . df.describe() . . Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . count 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1.674000e+03 | . mean 78.769415 | 61.448614 | 2014.002987 | 290.786081 | 65.667103 | 29.163542 | 58.120687 | 70.922877 | 631.125448 | 0.206691 | 2.337515 | 0.025408 | 0.147527 | 0.037019 | 0.065192 | 0.020192 | 1.488682e+06 | . std 14.241512 | 5.057758 | 2.609352 | 8.908379 | 2.743211 | 0.518966 | 3.386783 | 0.698738 | 452.741472 | 0.516601 | 2.060691 | 0.344145 | 0.695400 | 0.379702 | 0.380895 | 0.223493 | 1.410333e+06 | . min 45.000000 | 43.020000 | 2010.000000 | 266.400000 | 53.540000 | 27.510000 | 44.010000 | 68.698000 | 3.000000 | 0.000000 | 0.000000 | -1.475000 | -3.209000 | -1.717000 | -1.680000 | -0.930000 | 2.465000e+04 | . 25% 69.000000 | 57.955000 | 2012.000000 | 284.900000 | 63.832500 | 28.802500 | 55.902500 | 70.494250 | 322.000000 | 0.000000 | 1.000000 | -0.187750 | -0.260250 | -0.190250 | -0.180000 | -0.123000 | 5.656412e+05 | . 50% 80.000000 | 61.435000 | 2014.000000 | 290.500000 | 65.790000 | 29.140000 | 58.290000 | 70.904500 | 530.000000 | 0.000000 | 2.000000 | 0.040000 | 0.147000 | 0.055000 | 0.081000 | 0.022500 | 1.046144e+06 | . 75% 89.000000 | 64.910000 | 2016.000000 | 296.375000 | 67.587500 | 29.520000 | 60.420000 | 71.343750 | 813.750000 | 0.000000 | 3.000000 | 0.258500 | 0.568500 | 0.287750 | 0.314500 | 0.175750 | 1.892478e+06 | . max 120.000000 | 76.880000 | 2018.000000 | 319.700000 | 73.520000 | 31.000000 | 69.330000 | 74.400000 | 4169.000000 | 5.000000 | 14.000000 | 1.130000 | 2.406000 | 1.485000 | 1.533000 | 0.660000 | 1.203046e+07 | . Exploratory Data Analysis | # Looking at the distribution of data f, ax = plt.subplots(nrows = 6, ncols = 3, figsize=(20,20)) distribution = df.loc[:,df.columns!=&#39;Player Name&#39;].columns rows = 0 cols = 0 for i, column in enumerate(distribution): p = sns.distplot(df[column], ax=ax[rows][cols]) cols += 1 if cols == 3: cols = 0 rows += 1 . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . . From the distributions plotted, most of the graphs are normally distributed. However, we can observe that Money, Points, Wins, and Top 10s are all skewed to the right. This could be explained by the separation of the best players and the average PGA Tour player. The best players have multiple placings in the Top 10 with wins that allows them to earn more from tournaments, while the average player will have no wins and only a few Top 10 placings that prevent them from earning as much. . # Looking at the number of players with Wins for each year win = df.groupby(&#39;Year&#39;)[&#39;Wins&#39;].value_counts() win = win.unstack() win.fillna(0, inplace=True) # Converting win into ints win = win.astype(int) print(win) . Wins 0 1 2 3 4 5 Year 2010 166 21 5 0 0 0 2011 156 25 5 0 0 0 2012 159 26 4 1 0 0 2013 152 24 3 0 0 1 2014 142 29 3 2 0 0 2015 150 29 2 1 1 0 2016 152 28 4 1 0 0 2017 156 30 0 3 1 0 2018 158 26 5 3 0 0 . . From this table, we can see that most players end the year without a win. It&#39;s pretty rare to find a player that has won more than once! . players = win.apply(lambda x: np.sum(x), axis=1) percent_no_win = win[0]/players percent_no_win = percent_no_win*100 print(percent_no_win) . Year 2010 86.458333 2011 83.870968 2012 83.684211 2013 84.444444 2014 80.681818 2015 81.967213 2016 82.162162 2017 82.105263 2018 82.291667 dtype: float64 . # Plotting percentage of players without a win each year fig, ax = plt.subplots() bar_width = 0.8 opacity = 0.7 index = np.arange(2010, 2019) plt.bar(index, percent_no_win, bar_width, alpha = opacity) plt.xticks(index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;%&#39;) plt.title(&#39;Percentage of Players without a Win&#39;) . Text(0.5, 1.0, &#39;Percentage of Players without a Win&#39;) . . From the box plot above, we can observe that the percentages of players without a win are around 80%. There was very little variation in the percentage of players without a win in the past 8 years. . # Plotting the number of wins on a bar chart fig, ax = plt.subplots() index = np.arange(2010, 2019) bar_width = 0.2 opacity = 0.7 def plot_bar(index, win, labels): plt.bar(index, win, bar_width, alpha=opacity, label=labels) # Plotting the bars rects = plot_bar(index, win[0], labels = &#39;0 Wins&#39;) rects1 = plot_bar(index + bar_width, win[1], labels = &#39;1 Wins&#39;) rects2 = plot_bar(index + bar_width*2, win[2], labels = &#39;2 Wins&#39;) rects3 = plot_bar(index + bar_width*3, win[3], labels = &#39;3 Wins&#39;) rects4 = plot_bar(index + bar_width*4, win[4], labels = &#39;4 Wins&#39;) rects5 = plot_bar(index + bar_width*5, win[5], labels = &#39;5 Wins&#39;) plt.xticks(index + bar_width, index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Number of Players&#39;) plt.title(&#39;Distribution of Wins each Year&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f6f3b0236d0&gt; . . By looking at the distribution of Wins each year, we can see that it is rare for most players to even win a tournament in the PGA Tour. Majority of players do not win, and a very few number of players win more than once a year. . top10 = df.groupby(&#39;Year&#39;)[&#39;Top 10&#39;].value_counts() top10 = top10.unstack() top10.fillna(0, inplace=True) players = top10.apply(lambda x: np.sum(x), axis=1) no_top10 = top10[0]/players * 100 print(no_top10) . Year 2010 17.187500 2011 25.268817 2012 23.157895 2013 18.888889 2014 16.477273 2015 18.579235 2016 20.000000 2017 15.789474 2018 17.187500 dtype: float64 . By looking at the percentage of players that did not place in the top 10 by year, We can observe that only approximately 20% of players did not place in the Top 10. In addition, the range for these player that did not place in the Top 10 is only 9.47%. This tells us that this statistic does not vary much on a yearly basis. . distance = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Avg Distance&#39;]].copy() distance.sort_values(by=&#39;Avg Distance&#39;, inplace=True, ascending=False) print(distance.head()) . Year Player Name Avg Distance 162 2018 Rory McIlroy 319.7 1481 2011 J.B. Holmes 318.4 174 2018 Trey Mullinax 318.3 732 2015 Dustin Johnson 317.7 350 2017 Rory McIlroy 316.7 . Rory McIlroy is one of the longest hitters in the game, setting the average driver distance to be 319.7 yards in 2018. He was also the longest hitter in 2017 with an average of 316.7 yards. . money_ranking = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Money&#39;]].copy() money_ranking.sort_values(by=&#39;Money&#39;, inplace=True, ascending=False) print(money_ranking.head()) . Year Player Name Money 647 2015 Jordan Spieth 12030465.0 361 2017 Justin Thomas 9921560.0 303 2017 Jordan Spieth 9433033.0 729 2015 Jason Day 9403330.0 520 2016 Dustin Johnson 9365185.0 . We can see that Jordan Spieth has made the most amount of money in a year, earning a total of 12 million dollars in 2015. . # Who made the most money each year money_rank = money_ranking.groupby(&#39;Year&#39;)[&#39;Money&#39;].max() money_rank = pd.DataFrame(money_rank) indexs = np.arange(2010, 2019) names = [] for i in range(money_rank.shape[0]): temp = df.loc[df[&#39;Money&#39;] == money_rank.iloc[i,0],&#39;Player Name&#39;] names.append(str(temp.values[0])) money_rank[&#39;Player Name&#39;] = names print(money_rank) . Money Player Name Year 2010 4910477.0 Matt Kuchar 2011 6683214.0 Luke Donald 2012 8047952.0 Rory McIlroy 2013 8553439.0 Tiger Woods 2014 8280096.0 Rory McIlroy 2015 12030465.0 Jordan Spieth 2016 9365185.0 Dustin Johnson 2017 9921560.0 Justin Thomas 2018 8694821.0 Justin Thomas . . With this table, we can examine the earnings of each player by year. Some of the most notable were Jordan Speith&#39;s earning of 12 million dollars and Justin Thomas earning the most money in both 2017 and 2018. . # Plot the correlation matrix between variables corr = df.corr() sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, cmap=&#39;coolwarm&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6f3d00e390&gt; . . df.corr()[&#39;Wins&#39;] . Rounds 0.103162 Fairway Percentage -0.047949 Year 0.039006 Avg Distance 0.206294 gir 0.120340 Average Putts -0.168764 Average Scrambling 0.125193 Average Score -0.390254 Points 0.750110 Wins 1.000000 Top 10 0.473453 Average SG Putts 0.149155 Average SG Total 0.384932 SG:OTT 0.232414 SG:APR 0.259363 SG:ARG 0.134948 Money 0.721665 Name: Wins, dtype: float64 . From the correlation matrix, we can observe that Money is highly correlated to wins along with the FedExCup Points. We can also observe that the fairway percentage, year, and rounds are not correlated to Wins. . Machine Learning Model (Classification) | To predict winners, I used multiple machine learning models to explore which models could accurately classify if a player is going to win in that year. . To measure the models, I used Receiver Operating Characterisitc Area Under the Curve. (ROC AUC) The ROC AUC tells us how capable the model is at distinguishing players with a win. In addition, as the data is skewed with 83% of players having no wins in that year, ROC AUC is a much better metric than the accuracy of the model. . # Importing the Machine Learning modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, roc_auc_score from sklearn.metrics import confusion_matrix from sklearn.feature_selection import RFE from sklearn.metrics import classification_report from sklearn.preprocessing import PolynomialFeatures from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import MinMaxScaler . . Preparing the Data for Classification . We know from the calculation above that the data for wins is skewed. Even without machine learning we know that approximately 83% of the players does not lead to a win. Therefore, we will be utilizing ROC AUC as the metric of these models . df[&#39;Winner&#39;] = df[&#39;Wins&#39;].apply(lambda x: 1 if x&gt;0 else 0) # New DataFrame ml_df = df.copy() # Y value for machine learning is the Winner column target = df[&#39;Winner&#39;] # Removing the columns Player Name, Wins, and Winner from the dataframe to avoid leakage ml_df.drop([&#39;Player Name&#39;,&#39;Wins&#39;,&#39;Winner&#39;], axis=1, inplace=True) print(ml_df.head()) . Rounds Fairway Percentage Year ... SG:APR SG:ARG Money 0 60 75.19 2018 ... 0.960 -0.027 2680487.0 1 109 73.58 2018 ... 0.213 0.194 2485203.0 2 93 72.24 2018 ... 0.437 -0.137 2700018.0 3 78 71.94 2018 ... 0.532 0.273 1986608.0 4 103 71.44 2018 ... 0.099 0.026 1089763.0 [5 rows x 16 columns] . per_no_win = target.value_counts()[0] / (target.value_counts()[0] + target.value_counts()[1]) per_no_win = per_no_win.round(4)*100 print(str(per_no_win)+str(&#39;%&#39;)) . 83.09% . # Function for the logisitic regression def log_reg(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = LogisticRegression().fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Logistic regression classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Logistic regression classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features #rfe = RFE(clf, 5) # rfe = rfe.fit(X, y) # print(&#39;Feature Importance&#39;) # print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . . log_reg(ml_df, target) . . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 345 8 1 28 38 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.58 0.68 66 accuracy 0.91 419 macro avg 0.88 0.78 0.81 419 weighted avg 0.91 0.91 0.91 419 ROC AUC Score: 0.78 . From the logisitic regression, we got an accuracy of 0.9 on the training set and an accuracy of 0.91 on the test set. This was surprisingly accurate for a first run. However, the ROC AUC Score of 0.78 could be improved. Therefore, I decided to add more features as a way of possibly improving the model. . # Adding Domain Features ml_d = ml_df.copy() # Top 10 / Money might give us a better understanding on how well they placed in the top 10 ml_d[&#39;Top10perMoney&#39;] = ml_d[&#39;Top 10&#39;] / ml_d[&#39;Money&#39;] # Avg Distance / Fairway Percentage to give us a ratio that determines how accurate and far a player hits ml_d[&#39;DistanceperFairway&#39;] = ml_d[&#39;Avg Distance&#39;] / ml_d[&#39;Fairway Percentage&#39;] # Money / Rounds to see on average how much money they would make playing a round of golf ml_d[&#39;MoneyperRound&#39;] = ml_d[&#39;Money&#39;] / ml_d[&#39;Rounds&#39;] . log_reg(ml_d, target) . . Accuracy of Logistic regression classifier on training set: 0.91 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 342 11 1 27 39 precision recall f1-score support 0 0.93 0.97 0.95 353 1 0.78 0.59 0.67 66 accuracy 0.91 419 macro avg 0.85 0.78 0.81 419 weighted avg 0.90 0.91 0.90 419 ROC AUC Score: 0.78 . # Adding Polynomial Features to the ml_df mldf2 = ml_df.copy() poly = PolynomialFeatures(2) poly = poly.fit(mldf2) poly_feature = poly.transform(mldf2) print(poly_feature.shape) # Creating a DataFrame with the polynomial features poly_feature = pd.DataFrame(poly_feature, columns = poly.get_feature_names(ml_df.columns)) print(poly_feature.head()) . . (1674, 153) 1 Rounds Fairway Percentage ... SG:ARG^2 SG:ARG Money Money^2 0 1.0 60.0 75.19 ... 0.000729 -72373.149 7.185011e+12 1 1.0 109.0 73.58 ... 0.037636 482129.382 6.176234e+12 2 1.0 93.0 72.24 ... 0.018769 -369902.466 7.290097e+12 3 1.0 78.0 71.94 ... 0.074529 542343.984 3.946611e+12 4 1.0 103.0 71.44 ... 0.000676 28333.838 1.187583e+12 [5 rows x 153 columns] . log_reg(poly_feature, target) . . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 346 7 1 32 34 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.52 0.64 66 accuracy 0.91 419 macro avg 0.87 0.75 0.79 419 weighted avg 0.90 0.91 0.90 419 ROC AUC Score: 0.75 . From feature engineering, there were no improvements in the ROC AUC Score. In fact as I added more features, the accuracy and the ROC AUC Score decreased. This could signal to us that another machine learning algorithm could better predict winners. . ## Randon Forest Model def random_forest(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = RandomForestClassifier(n_estimators=200).fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Random Forest classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Random Forest classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features rfe = RFE(clf, 5) rfe = rfe.fit(X, y) print(&#39;Feature Importance&#39;) print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . . random_forest(ml_df, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 342 11 1 16 50 precision recall f1-score support 0 0.96 0.97 0.96 353 1 0.82 0.76 0.79 66 accuracy 0.94 419 macro avg 0.89 0.86 0.87 419 weighted avg 0.93 0.94 0.93 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Top 10&#39; &#39;Average SG Total&#39; &#39;Money&#39;] ROC AUC Score: 0.86 . random_forest(ml_d, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 343 10 1 16 50 precision recall f1-score support 0 0.96 0.97 0.96 353 1 0.83 0.76 0.79 66 accuracy 0.94 419 macro avg 0.89 0.86 0.88 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Average SG Total&#39; &#39;Money&#39; &#39;MoneyperRound&#39;] ROC AUC Score: 0.86 . random_forest(poly_feature, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 340 13 1 14 52 precision recall f1-score support 0 0.96 0.96 0.96 353 1 0.80 0.79 0.79 66 accuracy 0.94 419 macro avg 0.88 0.88 0.88 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Year Points&#39; &#39;Average Putts Points&#39; &#39;Average Scrambling Top 10&#39; &#39;Average Score Points&#39; &#39;Points^2&#39;] ROC AUC Score: 0.88 . The Random Forest Model scored highly on the ROC AUC Score, obtaining a value of 0.89. With this, we observed that the Random Forest Model could accurately classify players with and without a win. . Conclusion | It&#39;s been interesting to learn about numerous aspects of the game that differentiate the winner and the average PGA Tour player. For example, we can see that the fairway percentage and greens in regulations do not seem to contribute as much to a player&#39;s win. However, all the strokes gained statistics contribute pretty highly to wins for these players. It was interesting to see which aspects of the game that the professionals should put their time into. This also gave me the idea of track my personal golf statistics, so that I could compare it to the pros and find areas of my game that need the most improvement. . Machine Learning Model I&#39;ve been able to examine the data of PGA Tour players and classify if a player will win that year or not. With the random forest classification model, I was able to achieve an ROC AUC of 0.89 and an accuracy of 0.95 on the test set. This was a significant improvement from the ROC AUC of 0.78 and accuracy of 0.91. Because the data is skewed with approximately 80% of players not earning a win, the primary measure of the model was the ROC AUC. I was able to improve my model from ROC AUC score of 0.78 to a score of 0.89 by simply trying 3 different models, adding domain features, and polynomial features. . The End!! .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04_28_PGA_Wins.html",
            "relUrl": "/2021/04/30/_04_28_PGA_Wins.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Lambda School Data Science - Unit 1 Sprint 2",
            "content": "Autograded Notebook (Canvas &amp; CodeGrade) . This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully. . Instructions . Download this notebook as you would any other ipynb file | Upload to Google Colab or work locally (if you have that set-up) | Delete raise NotImplementedError() | Write your code in the # YOUR CODE HERE space | Execute the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas) | Save your notebook when you are finished | Download as a ipynb file (if working in Colab) | Upload your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas) | . Part A: Statistical Analysis . Use the following information to complete tasks 1 - 8 . Dataset description: . Anyone who is a fan of detective TV shows has watched a scene where human remains are discovered and some sort of expert is called in to determine when the person died. But is this science fiction or science fact? Is it possible to use evidence from skeletal remains to determine how long a body has been buried (a decent approximation of how long the person has been dead)? . Researchers sampled long bone material from bodies exhumed from coffin burials in two cemeteries in England. In each case, date of death and burial (and therefore interment time) was known. This data is given in the Longbones.csv dataset which you can find here. . What can we learn about the bodies that were buried in the cemetery? . The variable names are: . Site = Site ID, either Site 1 or Site 2 | Time = Interrment time in years | Depth = Burial depth in ft. | Lime = Burial with Quiklime (0 = No, 1 = Yes) | Age = Age at time of death in years | Nitro = Nitrogen composition of the long bones in g per 100g of bone. | Oil = Oil contamination of the grave site (0 = No contamination, 1 = Oil contamination) | . Source: D.R. Jarvis (1997). &quot;Nitrogen Levels in Long Bones from Coffin Burials Interred for Periods of 26-90 Years,&quot; Forensic Science International, Vol85, pp199-208 . Task 1 - Load the data . As we usually begin, let&#39;s load the data! The URL has been provided. . load your CSV file into a DataFrame named df | . import pandas as pd import numpy as np data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Longbones/Longbones.csv&#39; # YOUR CODE HERE df = pd.read_csv(data_url) # Print out your DataFrame df.head() . Site Time Depth Lime Age Nitro Oil . 0 1 | 88.5 | 7.0 | 1 | NaN | 3.88 | 1 | . 1 1 | 88.5 | NaN | 1 | NaN | 4.00 | 1 | . 2 1 | 85.2 | 7.0 | 1 | NaN | 3.69 | 1 | . 3 1 | 71.8 | 7.6 | 1 | 65.0 | 3.88 | 0 | . 4 1 | 70.6 | 7.5 | 1 | 42.0 | 3.53 | 0 | . Task 1 - Test . assert isinstance(df, pd.DataFrame), &#39;Have you created a DataFrame named `df`?&#39; assert len(df) == 42 . Task 2 - Missing data . Now, let&#39;s determine if there is any missing data in the dataset. If there is, drop the row that contains a missing value. . check for missing/null values and assign the sum to num_null - the result should be the sum of all the null values and a single integer (Hint: you will compute the sum of a sum) | if there are null values, drop them in place (your DataFrame should still be df) | . Hint: If you need to go back and update your DataFrame, read in the data again before calculating the null values . # Hint: Make sure to read in the data again if you re-do you Null calculation # YOUR CODE HERE num_null = df.isnull().sum().sum() df.dropna(inplace=True) print(num_null) . 0 . Task 2 - Test . # Hidden tests - you will see the results when you submit to Canvas . Use the following information to complete tasks 3 - 8 . The mean nitrogen composition in living individuals is 4.3g per 100g of bone. . We wish to use the Longbones sample to test the null hypothesis that the mean nitrogen composition per 100g of bone in the deceased is 4.3g (equal to that of living humans) vs the alternative hypothesis that the mean nitrogen composition per 100g of bone in the deceased is not 4.3g (not equal to that of living humans). . Task 3 - Statistical hypotheses . Write the null and alternative hypotheses described above. . This task will not be autograded - but it is part of completing the challenge. . Task 3 ANSWER: . $H_0: mu =$ 4.3g . $H_a: mu neq$ 4.3g . Task 4 - Statistical distributions . What is the appropriate test for these hypotheses? A t-test or a chi-square test? Explain your answer in a sentence or two. . This task will not be autograded - but it is part of completing the challenge. . Task 4 ANSWER: . We want a t-test because we&#39;re going to compare a mean from our sample data to a known value, 4.3g/100g. A chi-square test compares if two groups are related/correlated. . Task 5 - Hypothesis testing . Use a built-in Python function to conduct the statistical test you identified earlier. The scipy stats module has been imported. . Assign the t statistic to the variable t | Assign the p-value to the variable p | . Hint: Review the documentation to verify what it returns. You can assign the two variables in one step or two steps. . # Use this import for your calculation from scipy import stats # YOUR CODE HERE t, p = stats.ttest_1samp(df[&#39;Nitro&#39;], 4.3) print(t) print(p) . -16.525765821830365 8.097649978903554e-18 . Task 5 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 6 - Conclusion . What is the p-value for this hypothesis test. Do you reject or fail to reject the null hypothesis at the 0.05 level? . This task will not be autograded - but it is part of the project! . Task 6 ANSWER: . The p-value is 8.1*10^-18. Since that is less than the .05 significance level, we reject the null hypothesis and conclude that the alternative hypothesis is correct(that the mean nitrogen level per 100 g of bone in the deceased is not 4.3g) . Task 7 - Confidence Interval . Calculate a 95% confidence interval for the mean nitrogen composition in the longbones of a deceased individual using the t.interval function. . Assign the lower end of the confidence interval to the variable l | Assign the upper end of the confidence interval to the variable u | . Hint: You will need to calculate other statistics to complete the confidence interval calculation. These variables can be named whatever you like - just make sure to name your confidence interval variables as specified above. . # Use this import for your calculation from scipy.stats import t # YOUR CODE HERE mean = df[&#39;Nitro&#39;].mean() sd = df[&#39;Nitro&#39;].std() n = df[&#39;Nitro&#39;].count() se = sd/(n**(1/2)) l, u = t.interval(alpha=0.95, df=34, loc=mean, scale=se) print(l) print(u) . 3.734020952024922 3.8579790479750784 . Task 7 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 8 - Conclusion . Write an interpretation of your 95% confidence interval. . This task will not be autograded - but it is part of completing the challenge. . Task 8 ANSWER: . We are 95% sure that the mean nitrogen level per 100 g in the deceased is between 3.73 and 3.86 . Part B: A/B Testing . Use the following information to complete tasks 9 - 18 . A/B Testing and Udacity . Udacity is an online learning platform geared toward tech professionals who want to develop skills in programming, data science, etc. These classes are intensive - both for the students and instructors - and the learning experience is best when students are able to dedicate enough time to the classes and there is not a lot of student churn. . Udacity wished to determine if presenting potential students with a screen that would remind them of the time commitment involved in taking a class would decrease the enrollment of students who were unlikely to succeed in the class. . At the time of the experiment, when a student selected a course, she was taken to the course overview page and presented with two options: &quot;start free trial&quot;, and &quot;access course materials&quot;. . If the student clicked &quot;start free trial&quot;, she was asked to enter her credit card information and was enrolled in a free trial for the paid version of the course (which would covert to a paid membership after 14 days). . If the student clicked &quot;access course materials&quot;, she could view the videos and take the quizzes for free but could not access all the features of the course such as coaching. . Credit: Udacity A/B testing final project example . Here&#39;s the experiment: Udacity tested a change where if the student clicked &quot;start free trial&quot;, she was asked how much time she had available to devote to the course. . If the student indicated 5 or more hours per week, she would be taken through the checkout process as usual. If she indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion and suggesting that the student might like to access the course materials for free. . At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. . Now we wish to see if there was an association between the screen the potential student viewed and whether or not the student enrolled in the paid version of the course. . The Udacity data is linked below and is in a non-tidy format. We&#39;ll be focusing on the number of enrolling customers who convert to paying customers. . You don&#39;t need to do anything with the non-tidy data in this Challenge; we&#39;re sharing it here so you can get an idea of what data looks like before we clean it. . import pandas as pd import numpy as np # Load data data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Udacity%20AB%20testing%20data/AB%20testing%20data.csv&#39; ABtest_ = pd.read_csv(data_url) print(ABtest_.shape) ABtest_.head() . (999, 10) . Date C-Pageviews C-Clicks C-Enrollments C-Payments E-Pageviews E-Clicks E-Enrollments E-Payments Unnamed: 9 . 0 Sat, Oct 11 | 7723.0 | 687.0 | 134.0 | 70.0 | 7716.0 | 686.0 | 105.0 | 34.0 | NaN | . 1 Sun, Oct 12 | 9102.0 | 779.0 | 147.0 | 70.0 | 9288.0 | 785.0 | 116.0 | 91.0 | NaN | . 2 Mon, Oct 13 | 10511.0 | 909.0 | 167.0 | 95.0 | 10480.0 | 884.0 | 145.0 | 79.0 | NaN | . 3 Tue, Oct 14 | 9871.0 | 836.0 | 156.0 | 105.0 | 9867.0 | 827.0 | 138.0 | 92.0 | NaN | . 4 Wed, Oct 15 | 10014.0 | 837.0 | 163.0 | 64.0 | 9793.0 | 832.0 | 140.0 | 94.0 | NaN | . Now, here is the enrollment and payment data in tidy format. You can see how I set it up here. . data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Udacity%20AB%20testing%20data/AB_test_payments.csv&#39; ABtest = pd.read_csv(data_url, skipinitialspace=True, header=0) print(ABtest.shape) ABtest.head() . (7208, 3) . UserID Group Payment . 0 0 | Control | 1 | . 1 1 | Control | 1 | . 2 2 | Control | 1 | . 3 3 | Control | 1 | . 4 4 | Control | 1 | . Dataset information . The &quot;tidy&quot; data has the following values for the columns: . Group = Control or Experimental depending on the screen viewed | Payment = 0 if the individual did not not enroll as a paying customer, 1 = if the individual did enroll as a paying customer | . Our goal is to determine if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not she converted to a paying customer. . Task 9 - Statistical hypotheses . Write the null and alternative hypothesis to test if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not he or she converted to a paying customer. . This task will not be autograded - but it is part of completing the challenge. . Task 9 ANSWER: . Ho: There is no relationship between the screen viewed and converting to a paying customer. . Ha: There is a relationship between the screen viewed and conterving to a paying customer. . Task 10 - Frequency and relative frequency . Calculate the frequency and relative frequency of viewing the control version of the website and the experimental version of the website. . Use pd.crosstab() | Assign the frequency table the name group_freq | Assign the relative frequency table the name group_pct. Multiply by 100 to convert the proportions in the table to percents. | . # YOUR CODE HERE group_freq = ABtest[&#39;Group&#39;].value_counts() group_pct = ABtest[&#39;Group&#39;].value_counts(normalize = True)*100 #group_freq = pd.crosstab(ABtest[&#39;Group&#39;]) #group_pct = pd.crosstab(ABtest[&#39;Group&#39;],normalize = True)*100 print(group_freq) print(group_pct) . Control 3785 Experiment 3423 Name: Group, dtype: int64 Control 52.511099 Experiment 47.488901 Name: Group, dtype: float64 . Task 10 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 11 - Frequency and relative frequency . Calculate the frequency and relative frequency of converting to a paying customer. . Use pd.crosstab() | Assign the frequency table the name pay_freq | Assign the relative frequency table the name pay_pct. Multiply by 100 to convert the proportions in the table to percents. | . # YOUR CODE HERE pay_freq = ABtest[&#39;Payment&#39;].value_counts() pay_pct = ABtest[&#39;Payment&#39;].value_counts(normalize = True)*100 print(pay_freq) print(pay_pct) . 1 3978 0 3230 Name: Payment, dtype: int64 1 55.188679 0 44.811321 Name: Payment, dtype: float64 . Task 11 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 12 - Joint distribution . Calculate the joint distribution of experimental condition and conversion to a paying customer. . Use the experimental group as the index variable | Name the results of the joint distribution joint_dist | . # YOUR CODE HERE joint_dist = pd.crosstab(index = ABtest[&#39;Group&#39;], columns=ABtest[&#39;Payment&#39;]) joint_dist . Payment 0 1 . Group . Control 1752 | 2033 | . Experiment 1478 | 1945 | . Task 12 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 13 - Marginal distribution . Add the table margins to the joint distribution of experimental condition and conversion to a paying customer. . Use the experimental group as the index variable | Name the results of the distribution marginal_dist | . # YOUR CODE HERE marginal_dist = pd.crosstab(index = ABtest[&#39;Group&#39;], columns=ABtest[&#39;Payment&#39;], margins=True) marginal_dist . Payment 0 1 All . Group . Control 1752 | 2033 | 3785 | . Experiment 1478 | 1945 | 3423 | . All 3230 | 3978 | 7208 | . Task 13 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 14 - Conditional distribution . Calculate the distribution of payment conversion conditional on the text the individual saw when he or she was signing up for Udacity. . Use the experimental group as the index variable | Name the results of the distribution conditional_dist and make sure to multiple the result by 100 | . # YOUR CODE HERE conditional_dist = pd.crosstab(index=ABtest[&quot;Group&quot;], columns=ABtest[&quot;Payment&quot;],normalize=&quot;index&quot;)*100 conditional_dist . Payment 0 1 . Group . Control 46.287979 | 53.712021 | . Experiment 43.178498 | 56.821502 | . Task 14 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 15 - Statistical distributions . Identify the appropriate statistical test to determine if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not he or she converted to a paying customer. . This task will not be autograded - but it is part of completing the challenge. . Task 15 ANSWER: . It is appropriate to use the chi squared test because we&#39;re seeing if there is a correlation between two things(categorical variables), screen viewed and becoming a paying customer. . Task 16 - Hypothesis testing . Conduct the hypothesis test you identified in Task 15. . Assign the p-value to the variable p | . Hint: The chi2_contingency() function returns more than one parameter - make sure to read the documentation to assign the correct one to your p-value . from scipy.stats import chi2_contingency # YOUR CODE HERE g, p, dof, expctd = chi2_contingency(pd.crosstab(index=ABtest[&quot;Group&quot;], columns=ABtest[&quot;Payment&quot;])) print(p) . 0.008608736615463934 . Task 16 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 17 - Conclusions . Do you reject or fail to reject the null hypothesis at the 0.05 significance level? . This task will not be autograded - but it is part of completing the challenge. . Task 17 ANSWER: . 0.008&lt;0.05, so we reject the null hypothesis and conclude that the alternative hypothesis is correct, that there is a statistically significant relationship between the screen viewed and converting to a paying customer. . Task 18 - Visualization . Draw a side-by-side boxplot illustrating the conditional distribution of conversion by experimental group. . This task will not be autograded - but it is part of completing the challenge. . import matplotlib.pyplot as plt import seaborn as sns # YOUR CODE HERE sns.barplot(x=&#39;Group&#39;, y=&#39;Payment&#39;, data = ABtest, ci = None); . Task 19 - Bayesian and Frequentist Statistics . In a few sentences, describe the difference between Bayesian and Frequentist statistics. . This task will not be autograded - but it is part of completing the challenge. . Task 19 ANSWER: . Bayesian Statistics looks at prior belief in addition to data, whereas Frequentist Statistics only uses the data as a source of information. Frequentist uses fixed parameters and looks at long run frequency to determine the probability. Bayesian uses random variables and probability is defined by degree of belief. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04-30_LS_DS_Unit1_Sprint2.html",
            "relUrl": "/2021/04/30/_04-30_LS_DS_Unit1_Sprint2.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Does College Predict a Successful NBA Career?",
            "content": "The Relationship Between Attending a Power 6 Conference for Basketball and Having a Successful NBA Career . https://en.wikipedia.org/wiki/Air_Jordan — . The Intro . Have you ever wondered what are the odds that a player from a small college will have a successful professional career? Well, I have. So I tried to determine if attending a Power Six Conference for college basketball correlates to having a successful career in the NBA. . Many times watching the NBA or NFL, I am surprised when the players share the name of the college they attended. I am often expecting to hear nothing but Alabama or Oklahoma for football, and only Kentucky or Duke for basketball. But often, what I hear is not what I expected. So, I wanted to test to see if there is truly a correlation to having a long, successful career and the college attended. If there is, it would make sense to me. But maybe the big school only helps you get into the professional league, and from there everyone has as good of a shot at having a long career. We shall see. . The Background . Sam Bowie is a name you may or may not know. He was a 7’1 center at Kentucky. He was picked by the Trailblazers in the 1984 NBA draft, the second pick in the first round. He was picked BEFORE Michael Jordan. And he was one of the biggest NBA busts of all time. Tyler Hansborough was a more recent player for UNC, from 2005 to 2009. He was a star in the ACC and was the 13th pick in the 2009 draft to the Pacers. He bounced around from the Pacers to the Raptors to the Hornets for seven years with little playing time before moving to the basketball leagues in China, where he currently plays. On the other hand, the Celtics’ Larry Bird was one of the greatest players of all time, and he attended Indiana State University. . For a certain number of years attending college wasn’t mandatory, and several notable players were drafted straight from high school. A few of these players include Kevin Garnett (‘95), Kobe Bryant (‘96), and LeBron James (‘03). In 2005 the rules of the league were updated, and to be eligible for the draft one had to be 19 years old. So since then, and before 1995, every player had at least attended some college. It is likely that any player good enough to be drafted from high school would have attended a major conference. However, since we are only looking at the relationship between conference attended and lasting NBA career, this will not affect our data, as players that had no college experience will be dropped from the dataset. . For college basketball, the top conferences are known as the Power 6 Conferences. These conferences are the Big Ten, Big East, Big 12, ACC, SEC, and the Pac-12. We’ll see if attending one of these conferences correlates to a successful career. . . The Data . The dataset used was found on Kaggle, published by Omri Goldstein. It can be found here: www.kaggle.com/drgilermo/nba-players-stats. The data on this file was scraped from basketball-reference.com. The dataset contains aggregate individual statistics in the NBA since 1950, 67 seasons, from basic box-score attributes such as points, assists, and rebounds to more advanced money-ball like features such as Value Over Replacement. It does not contain data from seasons since 2017. . The Statistics . For determining if there is a relationship between attending a Power 6 Conference and having a successful NBA Career, we will use a Chi-Square Test of Independence. The hypotheses are as follows: . -Ho: There is no relationship between attending a Power 6 conference and having a successful NBA career. . -Ha: There is a relationship between attendeding a Power 6 conference and having a successful NBA career. . Data Wrangling // Feature Engineering . The dataset only showed the starting and ending year for each player. From here I subtracted the two to calculate the total years played for each player. From here, I found the average NBA career length to be 4.17 years. Then I created a new “Successful Career” to include players that played for at least double the amount of time as an average NBA career, so at least 9 years. Roughly 20% of the players received a “Yes” for “Successful Career”, so this seemed like a fitting measure. . I also created a label for Power Six Conference, and I applied to each row to find who attended a qualifying school. I dropped any row with NAN for college, as we only want to compare those that went. Of the 4248 players included in the dataset that attended a college, 2179 attended a Power 6 conference, while 2069 did not. So 51% of players have attended a Power 6 conference, with 49% going to smaller conferences or Division 2. . Statistical Methods . Below is a crosstab comparing those that did or did not attend a Power 6 conference and those that did or did not have a successful career. From this, the chi square test can be performed. . The Visualizations and Results . This barplot from Seaborn shows that the average number of years played for not attending a Power 6 is just under 4, and for attending a Power 6 is just over 4. . This catplot shows different career lengths and how many players correspond. A hue was added to show the difference between conference attended. . I used a chi square test of independence, the chi2_contingency function, to see if there was a correlation between Power 6 conference and a successful career in the NBA. After doing the test, the p-value is: 2.9795317795864165e-08. This is a very small p-value, so we would reject the null hypothesis and assume there is a statistically significant relationship between attending a Power 6 conference and having a successful NBA career. . This barplot shows that those that had a successful career more often attended a Power 6, about 60% of them do. . The Conclusion . There, in fact, is a relationship between attending a Power 6 Conference for basketball and having a successful NBA career. . Limitations . Some schools, like Gonzaga, are top programs, though they are in a smaller conference. Adam Morrison, from Gonzaga was one of the top picks in his draft year, although he did become a bust. So it’s possible that we could’ve included these select schools that are outside of the Power 6 conferences, but are still top programs. Also, we could’ve dropped the years from 1995 to 2005 where attending college wasn’t required to join the NBA, this could also alter our data. . Similar Analysis . A similar analysis by the NCAA showed the differences in being drafted to the NBA from all of Division 1 schools versus the top conferences. It found that 4.2% of draft-eligible Division I players were chosen in the 2019 NBA draft (52 / 1,224). It also showed that 18% of draft-eligible players from the ACC, Big Ten, Big 12, Pac-12, SEC, and Big East conferences were drafted by the NBA in 2019 (41 / 228). (NCAA. “Estimated Probability of Competing in Professional Athletics.” NCAA.org - The Official Site of the NCAA, 20 Apr. 2020, www.ncaa.org/about/resources/research/estimated-probability-competing-professional-athletics.) . Questions Raised . It’s possible that riding the bench at a Power 6 conference would not correlate as highly as being a starter on the team. Or, if we took out Division 2 and lower schools, there could be less of a difference in Power 6 Conferences to other Division 1 schools. We could break down the data further to understand these relationships. . The Recap . While there is no proven causation, there is a significant correlation with attending a major conference and having a long NBA career. So, I would recommend anyone interested in that to go to Duke. . Thank You. .",
            "url": "https://brennanashley.github.io/lambdalost/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "relUrl": "/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://brennanashley.github.io/lambdalost/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Welcome to Lambda Lost! I’m so happy you’re here. I’m Ash, a girl who codes. . I want to inspire other girls to enter the computer science field and enjoy the opportunities it offers. I enrolled in Lambda School’s data science program in January 2021, ready to jumpstart my career in coding. It can be overwhelming in the beginning, I’ve often felt lost myself, so we’ve got to help eachother out! I’ll be sharing my projects from the course and on the side, interesting data and computer science applications, and other things along the way! Follow along as I start this journey into the land of code with no prior experience, just a desire to learn something new! . XOXO, Ash . .",
          "url": "https://brennanashley.github.io/lambdalost/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://brennanashley.github.io/lambdalost/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}