{
  
    
        "post0": {
            "title": "Title",
            "content": "PGA Tour Classification Portfolio Project . Can We Predict If a PGA Tour Player Won a Tournament in That Year and Their Earnings? . Having grown up watching golf, I have always been interested in exploring what sets the best golfers (golfers with wins) apart from the rest. Therefore, I decided to explore their statistics. To collect all the data, I scraped the data from the PGA Tour website using python libraries such as beautifulsoup. (The code for the data collection is included in the repository) . From this data, I performed an exploratory data analysis to explore the distribution of players on numerous aspects of the game, discover outliers, and further explore how the game has changed from 2010 to 2018. I also utilized numerous supervised machine learning models to predict a golfer&#39;s earnings and wins. . To predict the golfer&#39;s win, I used classification methods such as logisitic regression and Random Forest Classification. I found that I had the best performance with the Random Forest Classification method. . Description of the Data Back to table of Contents | pgaTourData.csv contains 1674 rows and 18 columns. Each row indicates a golfer&#39;s performance for that year. . Player Name: Name of the golfer . Rounds: The number of games that a player played . Fairway Percentage: The percentage of time a tee shot lands on the fairway . Year: The year in which the statistic was collected . Avg Distance: The average distance of the tee-shot . gir: (Green in Regulation) is met if any part of the ball is touching the putting surface while the number of strokes taken is at least two fewer than par Average Putts: The average number of strokes taken on the green . Average Scrambling: Scrambling is when a player misses the green in regulation, but still makes par or better on a hole . Average Score: Average Score is the average of all the scores a player has played in that year . Points: The number of FedExCup points a player earned in that year. These points can be earned by competing in tournaments. . Wins: The number of competition a player has won in that year . Top 10: The number of competitions where a player has placed in the Top 10 . Average SG Putts: Strokes gained: putting measures how many strokes a player gains (or loses) on the greens. . Average SG Total: The Off-the-tee + approach-the-green + around-the-green + putting statistics combined . SG:OTT: Strokes gained: off-the-tee measures player performance off the tee on all par-4s and par-5s. . SG:APR: Strokes gained: approach-the-green measures player performance on approach shots. Approach shots include all shots that are not from the tee on par-4 and par-5 holes and are not included in strokes gained: around-the-green and strokes gained: putting. Approach shots include tee shots on par-3s. . SG:ARG: Strokes gained: around-the-green measures player performance on any shot within 30 yards of the edge of the green. This statistic does not include any shots taken on the putting green. . Money: The amount of prize money a player has earned from tournaments . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;pgaTourData.csv&#39;) # Examining the first 5 data df.head() . Player Name Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . 0 Henrik Stenson | 60.0 | 75.19 | 2018 | 291.5 | 73.51 | 29.93 | 60.67 | 69.617 | 868 | NaN | 5.0 | -0.207 | 1.153 | 0.427 | 0.960 | -0.027 | $2,680,487 | . 1 Ryan Armour | 109.0 | 73.58 | 2018 | 283.5 | 68.22 | 29.31 | 60.13 | 70.758 | 1,006 | 1.0 | 3.0 | -0.058 | 0.337 | -0.012 | 0.213 | 0.194 | $2,485,203 | . 2 Chez Reavie | 93.0 | 72.24 | 2018 | 286.5 | 68.67 | 29.12 | 62.27 | 70.432 | 1,020 | NaN | 3.0 | 0.192 | 0.674 | 0.183 | 0.437 | -0.137 | $2,700,018 | . 3 Ryan Moore | 78.0 | 71.94 | 2018 | 289.2 | 68.80 | 29.17 | 64.16 | 70.015 | 795 | NaN | 5.0 | -0.271 | 0.941 | 0.406 | 0.532 | 0.273 | $1,986,608 | . 4 Brian Stuard | 103.0 | 71.44 | 2018 | 278.9 | 67.12 | 29.11 | 59.23 | 71.038 | 421 | NaN | 3.0 | 0.164 | 0.062 | -0.227 | 0.099 | 0.026 | $1,089,763 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2312 entries, 0 to 2311 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 2312 non-null object 1 Rounds 1678 non-null float64 2 Fairway Percentage 1678 non-null float64 3 Year 2312 non-null int64 4 Avg Distance 1678 non-null float64 5 gir 1678 non-null float64 6 Average Putts 1678 non-null float64 7 Average Scrambling 1678 non-null float64 8 Average Score 1678 non-null float64 9 Points 2296 non-null object 10 Wins 293 non-null float64 11 Top 10 1458 non-null float64 12 Average SG Putts 1678 non-null float64 13 Average SG Total 1678 non-null float64 14 SG:OTT 1678 non-null float64 15 SG:APR 1678 non-null float64 16 SG:ARG 1678 non-null float64 17 Money 2300 non-null object dtypes: float64(14), int64(1), object(3) memory usage: 325.2+ KB . df.shape . (2312, 18) . Data Cleaning Back to table of Contents | From a rough look at the initial data, I realized that the data needs to be further cleaned. . For the columns Top 10 and Wins, convert the NaNs to 0s. Change Top 10 and Wins into an int Drop NaN values for players who do not have the full statistics Change the columns Rounds into int Change points to int Remove the dollar sign ($) and commas in the column Money . df[&#39;Top 10&#39;].fillna(0, inplace=True) df[&#39;Top 10&#39;] = df[&#39;Top 10&#39;].astype(int) # Replace NaN with 0 in # of wins df[&#39;Wins&#39;].fillna(0, inplace=True) df[&#39;Wins&#39;] = df[&#39;Wins&#39;].astype(int) # Drop NaN values df.dropna(axis = 0, inplace=True) . df[&#39;Rounds&#39;] = df[&#39;Rounds&#39;].astype(int) # Change Points to int df[&#39;Points&#39;] = df[&#39;Points&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Points&#39;] = df[&#39;Points&#39;].astype(int) # Remove the $ and commas in money df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;$&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].astype(float) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1674 entries, 0 to 1677 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 1674 non-null object 1 Rounds 1674 non-null int64 2 Fairway Percentage 1674 non-null float64 3 Year 1674 non-null int64 4 Avg Distance 1674 non-null float64 5 gir 1674 non-null float64 6 Average Putts 1674 non-null float64 7 Average Scrambling 1674 non-null float64 8 Average Score 1674 non-null float64 9 Points 1674 non-null int64 10 Wins 1674 non-null int64 11 Top 10 1674 non-null int64 12 Average SG Putts 1674 non-null float64 13 Average SG Total 1674 non-null float64 14 SG:OTT 1674 non-null float64 15 SG:APR 1674 non-null float64 16 SG:ARG 1674 non-null float64 17 Money 1674 non-null float64 dtypes: float64(12), int64(5), object(1) memory usage: 248.5+ KB . df.head() . Player Name Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . 0 Henrik Stenson | 60 | 75.19 | 2018 | 291.5 | 73.51 | 29.93 | 60.67 | 69.617 | 868 | 0 | 5 | -0.207 | 1.153 | 0.427 | 0.960 | -0.027 | 2680487.0 | . 1 Ryan Armour | 109 | 73.58 | 2018 | 283.5 | 68.22 | 29.31 | 60.13 | 70.758 | 1006 | 1 | 3 | -0.058 | 0.337 | -0.012 | 0.213 | 0.194 | 2485203.0 | . 2 Chez Reavie | 93 | 72.24 | 2018 | 286.5 | 68.67 | 29.12 | 62.27 | 70.432 | 1020 | 0 | 3 | 0.192 | 0.674 | 0.183 | 0.437 | -0.137 | 2700018.0 | . 3 Ryan Moore | 78 | 71.94 | 2018 | 289.2 | 68.80 | 29.17 | 64.16 | 70.015 | 795 | 0 | 5 | -0.271 | 0.941 | 0.406 | 0.532 | 0.273 | 1986608.0 | . 4 Brian Stuard | 103 | 71.44 | 2018 | 278.9 | 67.12 | 29.11 | 59.23 | 71.038 | 421 | 0 | 3 | 0.164 | 0.062 | -0.227 | 0.099 | 0.026 | 1089763.0 | . df.describe() . Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . count 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1.674000e+03 | . mean 78.769415 | 61.448614 | 2014.002987 | 290.786081 | 65.667103 | 29.163542 | 58.120687 | 70.922877 | 631.125448 | 0.206691 | 2.337515 | 0.025408 | 0.147527 | 0.037019 | 0.065192 | 0.020192 | 1.488682e+06 | . std 14.241512 | 5.057758 | 2.609352 | 8.908379 | 2.743211 | 0.518966 | 3.386783 | 0.698738 | 452.741472 | 0.516601 | 2.060691 | 0.344145 | 0.695400 | 0.379702 | 0.380895 | 0.223493 | 1.410333e+06 | . min 45.000000 | 43.020000 | 2010.000000 | 266.400000 | 53.540000 | 27.510000 | 44.010000 | 68.698000 | 3.000000 | 0.000000 | 0.000000 | -1.475000 | -3.209000 | -1.717000 | -1.680000 | -0.930000 | 2.465000e+04 | . 25% 69.000000 | 57.955000 | 2012.000000 | 284.900000 | 63.832500 | 28.802500 | 55.902500 | 70.494250 | 322.000000 | 0.000000 | 1.000000 | -0.187750 | -0.260250 | -0.190250 | -0.180000 | -0.123000 | 5.656412e+05 | . 50% 80.000000 | 61.435000 | 2014.000000 | 290.500000 | 65.790000 | 29.140000 | 58.290000 | 70.904500 | 530.000000 | 0.000000 | 2.000000 | 0.040000 | 0.147000 | 0.055000 | 0.081000 | 0.022500 | 1.046144e+06 | . 75% 89.000000 | 64.910000 | 2016.000000 | 296.375000 | 67.587500 | 29.520000 | 60.420000 | 71.343750 | 813.750000 | 0.000000 | 3.000000 | 0.258500 | 0.568500 | 0.287750 | 0.314500 | 0.175750 | 1.892478e+06 | . max 120.000000 | 76.880000 | 2018.000000 | 319.700000 | 73.520000 | 31.000000 | 69.330000 | 74.400000 | 4169.000000 | 5.000000 | 14.000000 | 1.130000 | 2.406000 | 1.485000 | 1.533000 | 0.660000 | 1.203046e+07 | . Exploratory Data Analysis | # Looking at the distribution of data f, ax = plt.subplots(nrows = 6, ncols = 3, figsize=(20,20)) distribution = df.loc[:,df.columns!=&#39;Player Name&#39;].columns rows = 0 cols = 0 for i, column in enumerate(distribution): p = sns.distplot(df[column], ax=ax[rows][cols]) cols += 1 if cols == 3: cols = 0 rows += 1 . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . . From the distributions plotted, It appears that most of the graphs are normally distributed. However, we can observe that Money, Points, Wins, and Top 10s tend to are all skewed to the right. This could be explained by the separation of the best players and the average PGA Tour player. The best players have multiple placings in the Top 10 with wins that allows them to earn more from tournaments, while the average player will have no wins and only a few Top 10 placings that prevent them from earning as much. . win = df.groupby(&#39;Year&#39;)[&#39;Wins&#39;].value_counts() win = win.unstack() win.fillna(0, inplace=True) # Converting win into ints win = win.astype(int) print(win) . Wins 0 1 2 3 4 5 Year 2010 166 21 5 0 0 0 2011 156 25 5 0 0 0 2012 159 26 4 1 0 0 2013 152 24 3 0 0 1 2014 142 29 3 2 0 0 2015 150 29 2 1 1 0 2016 152 28 4 1 0 0 2017 156 30 0 3 1 0 2018 158 26 5 3 0 0 . From this table, we can see that most players end the year without a win. In fact it is pretty rare to find a player that has won more than once. . players = win.apply(lambda x: np.sum(x), axis=1) percent_no_win = win[0]/players percent_no_win = percent_no_win*100 print(percent_no_win) . Year 2010 86.458333 2011 83.870968 2012 83.684211 2013 84.444444 2014 80.681818 2015 81.967213 2016 82.162162 2017 82.105263 2018 82.291667 dtype: float64 . fig, ax = plt.subplots() bar_width = 0.8 opacity = 0.7 index = np.arange(2010, 2019) plt.bar(index, percent_no_win, bar_width, alpha = opacity) plt.xticks(index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;%&#39;) plt.title(&#39;Percentage of Players without a Win&#39;) . Text(0.5, 1.0, &#39;Percentage of Players without a Win&#39;) . From the box plot above, we can observe that the percentages of players without a win are around 80%. There was also a negligible amount of variation in the percentage of players without a win in the past 8 years. . fig, ax = plt.subplots() index = np.arange(2010, 2019) bar_width = 0.2 opacity = 0.7 def plot_bar(index, win, labels): plt.bar(index, win, bar_width, alpha=opacity, label=labels) # Plotting the bars rects = plot_bar(index, win[0], labels = &#39;0 Wins&#39;) rects1 = plot_bar(index + bar_width, win[1], labels = &#39;1 Wins&#39;) rects2 = plot_bar(index + bar_width*2, win[2], labels = &#39;2 Wins&#39;) rects3 = plot_bar(index + bar_width*3, win[3], labels = &#39;3 Wins&#39;) rects4 = plot_bar(index + bar_width*4, win[4], labels = &#39;4 Wins&#39;) rects5 = plot_bar(index + bar_width*5, win[5], labels = &#39;5 Wins&#39;) plt.xticks(index + bar_width, index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Number of Wins&#39;) plt.title(&#39;Distribution of Wins each Year&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fefff16b510&gt; . By looking at the distribution of Wins each year, we can see that it is rare for most players to even win a tournament in the PGA Tour. Majority of players do not win, and a very few number of players win more than once a year. . top10 = df.groupby(&#39;Year&#39;)[&#39;Top 10&#39;].value_counts() top10 = top10.unstack() top10.fillna(0, inplace=True) players = top10.apply(lambda x: np.sum(x), axis=1) no_top10 = top10[0]/players * 100 print(no_top10) . Year 2010 17.187500 2011 25.268817 2012 23.157895 2013 18.888889 2014 16.477273 2015 18.579235 2016 20.000000 2017 15.789474 2018 17.187500 dtype: float64 . By looking at the percentage of players that did not place in the top 10 by year, We can observe that only approximately 20% of players did not place in the Top 10. In addition, the range for these player that did not place in the Top 10 is only 9.47%. This tells us that this statistic does not vary much on a yearly basis. . distance = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Avg Distance&#39;]].copy() distance.sort_values(by=&#39;Avg Distance&#39;, inplace=True, ascending=False) print(distance.head()) . Year Player Name Avg Distance 162 2018 Rory McIlroy 319.7 1481 2011 J.B. Holmes 318.4 174 2018 Trey Mullinax 318.3 732 2015 Dustin Johnson 317.7 350 2017 Rory McIlroy 316.7 . We can see that Rory McIlroy is one of the longest hitters in the game, setting the average driver distance to be 319.7 yards in 2018. He was also the longest hitter in 2017 with an average of 316.7 yards. There are other notable players like J.B. Holmes and Dustin Johnson who have an average of over 317 yards. . money_ranking = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Money&#39;]].copy() money_ranking.sort_values(by=&#39;Money&#39;, inplace=True, ascending=False) print(money_ranking.head()) . Year Player Name Money 647 2015 Jordan Spieth 12030465.0 361 2017 Justin Thomas 9921560.0 303 2017 Jordan Spieth 9433033.0 729 2015 Jason Day 9403330.0 520 2016 Dustin Johnson 9365185.0 . We can see that Jordan Spieth has made the most amount of money in a year. Earning an outstanding total of 12 million dollars . money_rank = money_ranking.groupby(&#39;Year&#39;)[&#39;Money&#39;].max() money_rank = pd.DataFrame(money_rank) print(money_rank.iloc[0,0]) indexs = np.arange(2010, 2019) names = [] for i in range(money_rank.shape[0]): temp = df.loc[df[&#39;Money&#39;] == money_rank.iloc[i,0],&#39;Player Name&#39;] names.append(str(temp.values[0])) money_rank[&#39;Player Name&#39;] = names print(money_rank) . 4910477.0 Money Player Name Year 2010 4910477.0 Matt Kuchar 2011 6683214.0 Luke Donald 2012 8047952.0 Rory McIlroy 2013 8553439.0 Tiger Woods 2014 8280096.0 Rory McIlroy 2015 12030465.0 Jordan Spieth 2016 9365185.0 Dustin Johnson 2017 9921560.0 Justin Thomas 2018 8694821.0 Justin Thomas . With this table, we can examine the earnings of each player by year. Some of the most notable were Jordan Speith&#39;s earning of 12 million dollars and Justin Thomas earning the most money in both 2017 and 2018. . corr = df.corr() sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, cmap=&#39;coolwarm&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff00140b850&gt; . df.corr()[&#39;Wins&#39;] . Rounds 0.103162 Fairway Percentage -0.047949 Year 0.039006 Avg Distance 0.206294 gir 0.120340 Average Putts -0.168764 Average Scrambling 0.125193 Average Score -0.390254 Points 0.750110 Wins 1.000000 Top 10 0.473453 Average SG Putts 0.149155 Average SG Total 0.384932 SG:OTT 0.232414 SG:APR 0.259363 SG:ARG 0.134948 Money 0.721665 Name: Wins, dtype: float64 . From the correlation matrix, we can observe that Money is highly correlated to wins along with the FedExCup Points. We can also observe that the fairway percentage, year, and rounds are not correlated to Wins. . Machine Learning Model (Classification) | To predict winners, I used multiple machine learning models to explore which models could accuracy classify if a player is going to win in that year. . To measure the models, I used Receiver Operating Characterisitc Area Under the Curve. (ROC AUC) The ROC AUC tells us how capable the model is at distinguishing players with a win. In addition, as the data is skewed with 83% of players having no wins in that year, ROC AUC is a better measure than the accuracy of the model. . from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, roc_auc_score from sklearn.metrics import confusion_matrix from sklearn.feature_selection import RFE from sklearn.metrics import classification_report from sklearn.preprocessing import PolynomialFeatures from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import MinMaxScaler # import warnings filter #from warnings import simplefilter # ignore all future warnings #simplefilter(action=&#39;ignore&#39;, category=FutureWarning) #if __name__ == &#39;__main__&#39;: # with warnings.catch_warnings(): # warnings.simplefilter(&#39;ignore&#39;, category=ImportWarning) . Preparing the Data for ClassificationÂ¶ We know from the calculation above that the data for wins is skewed. Even without machine learning we know that approximately 83% of the players does not lead to a win. Therefore, we will be utilizing ROC AUC as the primary measure of these models . df[&#39;Winner&#39;] = df[&#39;Wins&#39;].apply(lambda x: 1 if x&gt;0 else 0) # New DataFrame ml_df = df.copy() # Y value for machine learning is the Winner column target = df[&#39;Winner&#39;] # Removing the columns Player Name, Wins, and Winner from the dataframe ml_df.drop([&#39;Player Name&#39;,&#39;Wins&#39;,&#39;Winner&#39;], axis=1, inplace=True) print(ml_df.head()) . Rounds Fairway Percentage Year ... SG:APR SG:ARG Money 0 60 75.19 2018 ... 0.960 -0.027 2680487.0 1 109 73.58 2018 ... 0.213 0.194 2485203.0 2 93 72.24 2018 ... 0.437 -0.137 2700018.0 3 78 71.94 2018 ... 0.532 0.273 1986608.0 4 103 71.44 2018 ... 0.099 0.026 1089763.0 [5 rows x 16 columns] . per_no_win = target.value_counts()[0] / (target.value_counts()[0] + target.value_counts()[1]) per_no_win = per_no_win.round(4)*100 print(str(per_no_win)+str(&#39;%&#39;)) . 83.09% . def log_reg(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = LogisticRegression().fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Logistic regression classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Logistic regression classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features rfe = RFE(clf, 5) rfe = rfe.fit(X, y) print(&#39;Feature Importance&#39;) print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . log_reg(ml_df, target) . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 345 8 1 28 38 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.58 0.68 66 accuracy 0.91 419 macro avg 0.88 0.78 0.81 419 weighted avg 0.91 0.91 0.91 419 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . Feature Importance [&#39;Average Putts&#39; &#39;Top 10&#39; &#39;Average SG Total&#39; &#39;SG:OTT&#39; &#39;SG:APR&#39;] ROC AUC Score: 0.78 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . From the logisitic regression, we got an accuracy of 0.9 on the training set and an accuracy of 0.91 on the test set. This was surprisingly accurate for a first run. However, the ROC AUC Score of 0.78 could be improved. Therefore, I decided to add more features as a way of possibly improving the model. . # Adding Domain Features ml_d = ml_df.copy() # Top 10 / Money might give us a better understanding on how well they placed in the top 10 ml_d[&#39;Top10perMoney&#39;] = ml_d[&#39;Top 10&#39;] / ml_d[&#39;Money&#39;] # Avg Distance / Fairway Percentage to give us a ratio that determines how accurate and far a player hits ml_d[&#39;DistanceperFairway&#39;] = ml_d[&#39;Avg Distance&#39;] / ml_d[&#39;Fairway Percentage&#39;] # Money / Rounds to see on average how much money they would make playing a round of golf ml_d[&#39;MoneyperRound&#39;] = ml_d[&#39;Money&#39;] / ml_d[&#39;Rounds&#39;] . log_reg(ml_d, target) . Accuracy of Logistic regression classifier on training set: 0.91 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 342 11 1 27 39 precision recall f1-score support 0 0.93 0.97 0.95 353 1 0.78 0.59 0.67 66 accuracy 0.91 419 macro avg 0.85 0.78 0.81 419 weighted avg 0.90 0.91 0.90 419 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . Feature Importance [&#39;Average Putts&#39; &#39;Top 10&#39; &#39;Average SG Total&#39; &#39;SG:OTT&#39; &#39;SG:APR&#39;] ROC AUC Score: 0.78 . mldf2 = ml_df.copy() poly = PolynomialFeatures(2) poly = poly.fit(mldf2) poly_feature = poly.transform(mldf2) print(poly_feature.shape) # Creating a DataFrame with the polynomial features poly_feature = pd.DataFrame(poly_feature, columns = poly.get_feature_names(ml_df.columns)) print(poly_feature.head()) . (1674, 153) 1 Rounds Fairway Percentage ... SG:ARG^2 SG:ARG Money Money^2 0 1.0 60.0 75.19 ... 0.000729 -72373.149 7.185011e+12 1 1.0 109.0 73.58 ... 0.037636 482129.382 6.176234e+12 2 1.0 93.0 72.24 ... 0.018769 -369902.466 7.290097e+12 3 1.0 78.0 71.94 ... 0.074529 542343.984 3.946611e+12 4 1.0 103.0 71.44 ... 0.000676 28333.838 1.187583e+12 [5 rows x 153 columns] . log_reg(poly_feature, target) . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 346 7 1 32 34 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.52 0.64 66 accuracy 0.91 419 macro avg 0.87 0.75 0.79 419 weighted avg 0.90 0.91 0.90 419 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . Feature Importance [&#39;Fairway Percentage Year&#39; &#39;Year^2&#39; &#39;Year gir&#39; &#39;Year Average Scrambling&#39; &#39;Year Average Score&#39;] ROC AUC Score: 0.75 . From feature engineering, there were no improvements in the ROC AUC Score. In fact as I added more features, the accuracy and the ROC AUC Score decreased. This could signal to us that another machine learning algorithm could better predict winners. . def random_forest(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = RandomForestClassifier(n_estimators=200).fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Random Forest classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Random Forest classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features rfe = RFE(clf, 5) rfe = rfe.fit(X, y) print(&#39;Feature Importance&#39;) print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . random_forest(ml_df, target) . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 342 11 1 14 52 precision recall f1-score support 0 0.96 0.97 0.96 353 1 0.83 0.79 0.81 66 accuracy 0.94 419 macro avg 0.89 0.88 0.89 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Top 10&#39; &#39;Average SG Total&#39; &#39;Money&#39;] ROC AUC Score: 0.88 . random_forest(ml_d, target) . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.95 0 1 0 343 10 1 13 53 precision recall f1-score support 0 0.96 0.97 0.97 353 1 0.84 0.80 0.82 66 accuracy 0.95 419 macro avg 0.90 0.89 0.89 419 weighted avg 0.94 0.95 0.94 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Average SG Total&#39; &#39;Money&#39; &#39;MoneyperRound&#39;] ROC AUC Score: 0.89 . random_forest(poly_feature, target) . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 342 11 1 13 53 precision recall f1-score support 0 0.96 0.97 0.97 353 1 0.83 0.80 0.82 66 accuracy 0.94 419 macro avg 0.90 0.89 0.89 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Year Points&#39; &#39;Average Putts Points&#39; &#39;Average Scrambling Top 10&#39; &#39;Average Score Points&#39; &#39;Points^2&#39;] ROC AUC Score: 0.89 . The Random Forest Model was scored highly on ROC AUC Score, obtaining a value of 0.89. With this, we observed that the Random Forest Model and the Support Vector Machine Models could accurately classify players with and without a win. . Conclusion | What I Learned From this notebook, I learned about numerous aspects of the game that differentiate the winner and the average PGA Tour player. For example, we can see that the fairway percentage and greens in regulations does not seems to contribute as much to a player&#39;s win. However, all the strokes gained statistics contribute pretty highly to wins for these players. It was interesting to see which aspects of the game that the professionals should put their time into. This also gave me the idea of track my personal golf statistics, so that I could compare it to the pros and find areas of my game that need the most improvement. . Machine Learning Model From this PGA Tour EDA and Machine Learning Models, I was able to examine the data of PGA Tour players, classify if a player will win that year or not, and predict their earnings. While, I believe that I can improve my prediction of their earnings, I am satisfied with my classification and regression model. With the random forest classification model, I was able to achieve an ROC AUC of 0.89 and an accuracy of 0.95 on the test set. This was a significant improvement from the ROC AUC of 0.78 and accuracy of 0.91. Because the data is skewed with approximately 80% of players not earning a win, the primary measure of the model was the ROC AUC. I was able to improve my model from ROC AUC score of 0.78 to a score of 0.89 by simply trying 3 different models, adding domain features, and polynomial features. . The End. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/23/PGA-Wins.html",
            "relUrl": "/2021/04/23/PGA-Wins.html",
            "date": " â¢ Apr 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Does College Predict a Successful NBA Career?",
            "content": "The Relationship Between Attending a Power 6 Conference for Basketball and Having a Successful NBA Career . https://en.wikipedia.org/wiki/Air_Jordan â . The Intro . Have you ever wondered what are the odds that a player from a small college will have a successful professional career? Well, I have. So I tried to determine if attending a Power Six Conference for college basketball correlates to having a successful career in the NBA. . Many times watching the NBA or NFL, I am surprised when the players share the name of the college they attended. I am often expecting to hear nothing but Alabama or Oklahoma for football, and only Kentucky or Duke for basketball. But often, what I hear is not what I expected. So, I wanted to test to see if there is truly a correlation to having a long, successful career and the college attended. If there is, it would make sense to me. But maybe the big school only helps you get into the professional league, and from there everyone has as good of a shot at having a long career. We shall see. . The Background . Sam Bowie is a name you may or may not know. He was a 7â1 center at Kentucky. He was picked by the Trailblazers in the 1984 NBA draft, the second pick in the first round. He was picked BEFORE Michael Jordan. And he was one of the biggest NBA busts of all time. Tyler Hansborough was a more recent player for UNC, from 2005 to 2009. He was a star in the ACC and was the 13th pick in the 2009 draft to the Pacers. He bounced around from the Pacers to the Raptors to the Hornets for seven years with little playing time before moving to the basketball leagues in China, where he currently plays. On the other hand, the Celticsâ Larry Bird was one of the greatest players of all time, and he attended Indiana State University. . For a certain number of years attending college wasnât mandatory, and several notable players were drafted straight from high school. A few of these players include Kevin Garnett (â95), Kobe Bryant (â96), and LeBron James (â03). In 2005 the rules of the league were updated, and to be eligible for the draft one had to be 19 years old. So since then, and before 1995, every player had at least attended some college. It is likely that any player good enough to be drafted from high school would have attended a major conference. However, since we are only looking at the relationship between conference attended and lasting NBA career, this will not affect our data, as players that had no college experience will be dropped from the dataset. . For college basketball, the top conferences are known as the Power 6 Conferences. These conferences are the Big Ten, Big East, Big 12, ACC, SEC, and the Pac-12. Weâll see if attending one of these conferences correlates to a successful career. . . The Data . The dataset used was found on Kaggle, published by Omri Goldstein. It can be found here: www.kaggle.com/drgilermo/nba-players-stats. The data on this file was scraped from basketball-reference.com. The dataset contains aggregate individual statistics in the NBA since 1950, 67 seasons, from basic box-score attributes such as points, assists, and rebounds to more advanced money-ball like features such as Value Over Replacement. It does not contain data from seasons since 2017. . The Statistics . For determining if there is a relationship between attending a Power 6 Conference and having a successful NBA Career, we will use a Chi-Square Test of Independence. The hypotheses are as follows: . -Ho: There is no relationship between attending a Power 6 conference and having a successful NBA career. . -Ha: There is a relationship between attendeding a Power 6 conference and having a successful NBA career. . Data Wrangling // Feature Engineering . The dataset only showed the starting and ending year for each player. From here I subtracted the two to calculate the total years played for each player. From here, I found the average NBA career length to be 4.17 years. Then I created a new âSuccessful Careerâ to include players that played for at least double the amount of time as an average NBA career, so at least 9 years. Roughly 20% of the players received a âYesâ for âSuccessful Careerâ, so this seemed like a fitting measure. . I also created a label for Power Six Conference, and I applied to each row to find who attended a qualifying school. I dropped any row with NAN for college, as we only want to compare those that went. Of the 4248 players included in the dataset that attended a college, 2179 attended a Power 6 conference, while 2069 did not. So 51% of players have attended a Power 6 conference, with 49% going to smaller conferences or Division 2. . Statistical Methods . Below is a crosstab comparing those that did or did not attend a Power 6 conference and those that did or did not have a successful career. From this, the chi square test can be performed. . The Visualizations and Results . This barplot from Seaborn shows that the average number of years played for not attending a Power 6 is just under 4, and for attending a Power 6 is just over 4. . This catplot shows different career lengths and how many players correspond. A hue was added to show the difference between conference attended. . I used a chi square test of independence, the chi2_contingency function, to see if there was a correlation between Power 6 conference and a successful career in the NBA. After doing the test, the p-value is: 2.9795317795864165e-08. This is a very small p-value, so we would reject the null hypothesis and assume there is a statistically significant relationship between attending a Power 6 conference and having a successful NBA career. . This barplot shows that those that had a successful career more often attended a Power 6, about 60% of them do. . The Conclusion . There, in fact, is a relationship between attending a Power 6 Conference for basketball and having a successful NBA career. . Limitations . Some schools, like Gonzaga, are top programs, though they are in a smaller conference. Adam Morrison, from Gonzaga was one of the top picks in his draft year, although he did become a bust. So itâs possible that we couldâve included these select schools that are outside of the Power 6 conferences, but are still top programs. Also, we couldâve dropped the years from 1995 to 2005 where attending college wasnât required to join the NBA, this could also alter our data. . Similar Analysis . A similar analysis by the NCAA showed the differences in being drafted to the NBA from all of Division 1 schools versus the top conferences. It found that 4.2% of draft-eligible Division I players were chosen in the 2019 NBA draft (52 / 1,224). It also showed that 18% of draft-eligible players from the ACC, Big Ten, Big 12, Pac-12, SEC, and Big East conferences were drafted by the NBA in 2019 (41 / 228). (NCAA. âEstimated Probability of Competing in Professional Athletics.â NCAA.org - The Official Site of the NCAA, 20 Apr. 2020, www.ncaa.org/about/resources/research/estimated-probability-competing-professional-athletics.) . Questions Raised . Itâs possible that riding the bench at a Power 6 conference would not correlate as highly as being a starter on the team. Or, if we took out Division 2 and lower schools, there could be less of a difference in Power 6 Conferences to other Division 1 schools. We could break down the data further to understand these relationships. . The Recap . While there is no proven causation, there is a significant correlation with attending a major conference and having a long NBA career. So, I would recommend anyone interested in that to go to Duke. . Thank You. .",
            "url": "https://brennanashley.github.io/lambdalost/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "relUrl": "/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "date": " â¢ Feb 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â© . 2. This is the other footnote. You can even have a link!â© .",
            "url": "https://brennanashley.github.io/lambdalost/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Welcome to Lambda Lost! Iâm so happy youâre here. Iâm Ash, a girl who codes. . I want to inspire other girls to enter the computer science field and enjoy the opportunities it offers. I enrolled in Lambda Schoolâs data science program in January 2021, ready to jumpstart my career in coding. It can be overwhelming in the beginning, Iâve often felt lost myself, so weâve got to help eachother out! Iâll be sharing my projects from the course and on the side, interesting data and computer science applications, and other things along the way! Follow along as I start this journey into the land of code with no prior experience, just a desire to learn something new! . XOXO, Ash . .",
          "url": "https://brennanashley.github.io/lambdalost/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ âsitemap.xmlâ | absolute_url }} | .",
          "url": "https://brennanashley.github.io/lambdalost/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}