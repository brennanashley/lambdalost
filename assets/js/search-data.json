{
  
    
        "post0": {
            "title": "Lambda School Data Science - Unit 1 Sprint 3",
            "content": "Autograded Notebook (Canvas &amp; CodeGrade) . This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully. . Instructions . Download this notebook as you would any other ipynb file | Upload to Google Colab or work locally (if you have that set-up) | Delete raise NotImplementedError() | Write your code in the # YOUR CODE HERE space | Execute the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas) | Save your notebook when you are finished | Download as a ipynb file (if working in Colab) | Upload your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas) | . Welcome to the final Sprint Challenge of Unit 1! . In this challenge, we&#39;re going to explore two different datasets where you can demonstrate your skills with fitting linear regression models and practicing some of the linear algebra concepts you learned. . Make sure to follow the instructions in each task carefully! The autograded tests are very specific in that they are designed to test on the exact instructions. . Good luck! . Part A: Linear Regression . Use the following information to complete Tasks 1 - 11 . Dataset description . The data you will work on for this Sprint Challenge is from the World Happiness Report. The report compiles data from a survey of hundreds of countries and looks at factors such as economic production, social support, life expectancy, freedom, absence of corruption, and generosity to determine a happiness &quot;score&quot;. . In this Sprint Challenge, we&#39;re only going to look at the report for years 2018 and 2019. We&#39;re going to see how much the happiness &quot;score&quot; depends on some of the factors listed above. . For more information about the data, you can look here: Kaggle: World Happiness Report . Task 1 - Load the data . import both pandas and numpy | use the URL provided to read in your DataFrame | load the CSV file as a DataFrame with the name happy and set the index column as Overall_rank. | the shape of your DataFrame should be (312, 8) | . # URL provided url = &quot;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Happy/happiness_years18_19.csv&quot; # YOUR CODE HERE import pandas as pd import numpy as np happy = pd.read_csv(url, index_col=&#39;Overall_rank&#39;) # Print out the DataFrame happy.head() . Country_region Score GDP_per_capita Social_support Healthy_life_expectancy Freedom_life_choices Generosity Perceptions_corruption . Overall_rank . 1 Finland | 7.632 | 1.305 | 1.592 | 0.874 | 0.681 | 0.202 | 0.393 | . 2 Norway | 7.594 | 1.456 | 1.582 | 0.861 | 0.686 | 0.286 | NaN | . 3 Denmark | 7.555 | 1.351 | 1.590 | 0.868 | 0.683 | 0.284 | NaN | . 4 Iceland | 7.495 | 1.343 | 1.644 | 0.914 | 0.677 | 0.353 | 0.138 | . 5 Switzerland | 7.487 | 1.420 | 1.549 | 0.927 | 0.660 | 0.256 | 0.357 | . Task 1 - Test . assert isinstance(happy, pd.DataFrame), &#39;Have you created a DataFrame named `happy`?&#39; assert happy.index.name == &#39;Overall_rank&#39;, &quot;Your index should be &#39;Overall_rank&#39;.&quot; assert len(happy) == 312 . Task 2 - Explore the data and find NaNs . Now you want to take a look at the dataset, determine the variable types of the columns, identify missing values, and generally better understand your data. . Your tasks . Use describe() and info() to learn about any missing values, the data types, and descriptive statistics for each numeric value | Sum the null values and assign that number to the variable num_null; the variable type should be a numpy.int64 integer. | . Hint: If you use np.isnull() it will return the number of null values in each column. You want the total number of null values in the entire DataFrame; one way to do this is to apply the .sum() method twice: .sum().sum() . # YOUR CODE HERE happy.describe() happy.info() num_null = happy.isnull().sum().sum() # Print out your integer result print(&quot;The total number of null values is:&quot;, num_null) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 312 entries, 1 to 156 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 Country_region 312 non-null object 1 Score 312 non-null float64 2 GDP_per_capita 312 non-null float64 3 Social_support 312 non-null float64 4 Healthy_life_expectancy 312 non-null float64 5 Freedom_life_choices 312 non-null float64 6 Generosity 312 non-null float64 7 Perceptions_corruption 86 non-null float64 dtypes: float64(7), object(1) memory usage: 21.9+ KB The total number of null values is: 226 . Task 2 Test . import numpy as np assert isinstance(num_null, np.int64), &#39;The sum of the NaN values should be an integer.&#39; . Task 3 - Drop a column . As you noticed in the previous task, the column Perceptions_corruption has a lot of missing values. Let&#39;s determine how many are missing and then drop the column. Note: dropping a column isn&#39;t always the best choice when faced with missing values but we&#39;re choosing that option here, partly for practice. . Calculate the percentage of NaN values in Perceptions_corruption and assign the result to the variable corruption_nan; the value should be a float between 1.0 and 100.0. | Drop the Perceptions_corruption column from happy but keep the DataFrame name the same; use the parameter inplace=True. You will also want to specify the axis on which to operate. | . # YOUR CODE HERE corruption_nan = ((happy[&#39;Perceptions_corruption&#39;].isnull().sum())/(len(happy[&#39;Perceptions_corruption&#39;])))*100 happy.drop([&#39;Perceptions_corruption&#39;], axis=1, inplace=True) # Print the percentage of NaN values print(corruption_nan) # Print happy to verify the column was dropped happy.head() . 72.43589743589743 . Country_region Score GDP_per_capita Social_support Healthy_life_expectancy Freedom_life_choices Generosity . Overall_rank . 1 Finland | 7.632 | 1.305 | 1.592 | 0.874 | 0.681 | 0.202 | . 2 Norway | 7.594 | 1.456 | 1.582 | 0.861 | 0.686 | 0.286 | . 3 Denmark | 7.555 | 1.351 | 1.590 | 0.868 | 0.683 | 0.284 | . 4 Iceland | 7.495 | 1.343 | 1.644 | 0.914 | 0.677 | 0.353 | . 5 Switzerland | 7.487 | 1.420 | 1.549 | 0.927 | 0.660 | 0.256 | . *Task 3 Test . assert isinstance(corruption_nan, np.float), &#39;The percentage of NaN values should be a float.&#39; assert corruption_nan &gt;= 1, &#39;Make sure you calculated the percentage and not the decimal fraction.&#39; . Task 4 - Visualize the dataset . Next, we&#39;ll create a visualization for this dataset. We know from the introduction that we&#39;re trying to predict the happiness score from the other factors. Before we do let, let&#39;s visualize the dataset using a seaborn pairplot to look at all of the columns plotted as &quot;pairs&quot;. . Your tasks . Use the seaborn library sns.pairplot() function to create your visualization (use the starter code provided) | . This task will not be autograded - but it is part of completing the challenge. . # (NOT autograded but fill in your code!) # Import seaborn import seaborn as sns # Use sns.pairplot(data) where data is the name of your DataFrame # sns.pairplot() # YOUR CODE HERE sns.pairplot(happy) . &lt;seaborn.axisgrid.PairGrid at 0x7f649a065320&gt; . Task 5 - Identify the dependent and independent variables . Before we fit a linear regression to the variables in this data set, we need to determine the dependent variable (the target or y variable) and independent variable (the feature or x variable). For this dataset, we have one dependent variable and a few choices for the independent variable(s). Using the information about the data set and what you know from previous tasks, complete the following: . Assign the dependent variable to y_var | Choose one independent variable and assign it to x_var | . # YOUR CODE HERE y_var = happy[&#39;Score&#39;] x_var = happy[&#39;GDP_per_capita&#39;] . Task 5 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 6 - Fit a line using seaborn . Before we fit the linear regression model, we&#39;ll check how well a line fits. Because you have some choices for which independent variable to select, we&#39;re going to complete the rest of our analysis using GDP per capita as the independent variable. We&#39;re using Score as the dependent (target) variable. . The seaborn lmplot() documentation can be found here. You can also use regplot() and the documentation is here . This task will not be autograded - but it is part of completing the challenge! . Your tasks: . Create a scatter plot using seaborn with GDP per capita and Score | Use sns.lmplot() or sns.regplot() and specify a confidence interval of 0.95 | Answer the questions about your plot (not autograded). | . # YOUR CODE HERE sns.regplot(x=&quot;GDP_per_capita&quot;, y=&quot;Score&quot;, data=happy); . Task 6 - Short answer . Does it make sense to fit a linear model to these two variables? In otherwords, are there any problems with this data like extreme outliers, non-linearity, etc. | Over what range of your independent variable does the linear model not fit the data well? Over what range does a line fit the data well? | . Yes, it makes sense to fit the linear model. There are relatively few outliers, though there are some at the very highest and lowest levels of GDP_per_capita. | Right at 0 for the independent variable isn&#39;t the greatest fit, and also above 2 for GDP_per_capita the data isn&#39;t very close to the line of best fit. Between 0.1 and 1.75 the independent variable fits the linear model well. | Task 7 - Fit a linear regression model . Now it&#39;s time to fit the linear regression model! We have two variables (GDP_per_capita and Score) that we are going to use in our model. . Your tasks: . Use the provided import for the statsmodels.formula.api library ols method | Fit a single variable linear regression model and assign the model to the variable model_1 | Print out the model summary and assign the value of R-squared for this model to r_square_model_1. Your value should be defined to three decimal places (example: r_square_model_1 = 0.123) | Answer the questions about your resulting model parameters (these short answer questions will not be autograded). | . NOTE: - For this task to be correctly autograded, you need to input the model parameters as specified in the code cell below. Part of this Sprint Challenge is correctly implementing the instructions in each task. . # Import the OLS model from statsmodels from statsmodels.formula.api import ols # YOUR CODE HERE model_1 = ols(&#39;Score~GDP_per_capita&#39;, data=happy).fit() # Print the model summary print(model_1.summary()) r_square_model_1 = 0.637 . OLS Regression Results ============================================================================== Dep. Variable: Score R-squared: 0.637 Model: OLS Adj. R-squared: 0.636 Method: Least Squares F-statistic: 543.4 Date: Sat, 20 Feb 2021 Prob (F-statistic): 3.82e-70 Time: 02:55:27 Log-Likelihood: -318.08 No. Observations: 312 AIC: 640.2 Df Residuals: 310 BIC: 647.7 Df Model: 1 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] - Intercept 3.3667 0.095 35.496 0.000 3.180 3.553 GDP_per_capita 2.2541 0.097 23.312 0.000 2.064 2.444 ============================================================================== Omnibus: 1.623 Durbin-Watson: 1.376 Prob(Omnibus): 0.444 Jarque-Bera (JB): 1.629 Skew: -0.113 Prob(JB): 0.443 Kurtosis: 2.727 Cond. No. 4.77 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Task 7 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 8 - Interpret your model . Using the model summary you printed out above, answer the following questions. . Assign the slope of GDP_per_capita to the variable slope_model_1; define it to two decimal places (example: 1.23). This variable should be a float. | Assign the p-value for this model parameter to pval_model_1; this variable could be either an integer or a float. | Assign the 95% confidence interval to the variables ci_low (lower value) and ci_upper (upper value); define them to two decimal places. | . Answer the following questions (not autograded): . Is the correlation between your variables positive or negative? | How would you write the confidence interval for your slope coefficient? | State the null hypothesis to test for a statistically significant relationship between your two variables. | Using the p-value from your model, do you reject or fail to reject the null hypothesis? | . Positive correlation | 95% between 2.064 and 2.444 | Ho: $ beta_1$ = 0 , Ha: $ beta_1 neq$ 0 | 0&lt;0.05, so we&#39;d reject the null hypothesis | # YOUR CODE HERE slope_model_1 = 2.25 pval_model_1 = 0.00 ci_low = 2.06 ci_upper = 2.44 . Task 8 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 9 - Fit a multiple predictor linear regression model . For this next task, we&#39;ll add in an additional independent or predictor variable. Let&#39;s look back at the pairplot and choose another variable - we&#39;ll use Social_support. Recall from the Guided Projects and Module Projects that we are looking to see if adding the variable Social_support is statistically significant after accounting for the GDP_per_capita variable. . We&#39;re going to fit a linear regression model using two predictor variables: GDP_per_capita and Social_support. . Your tasks: . Fit a model with both predictor variables and assign the model to model_2. The format of the input to the model is Y ~ X1 + X2. X1 = GDP_per_capita and X2 = Social_support. | . | Print out the model summary and assign the value of R-squared for this model to r_square_model_2. Your value should be defined to three decimal places. | Assign the value of the adjusted R-square to adj_r_square_model_2. Your value should be defined to three decimal places. | Answer the questions about your resulting model parameters (these short answer questions will not be autograded) | . # YOUR CODE HERE model_2 = ols(&#39;Score~GDP_per_capita+Social_support&#39;, data=happy).fit() r_square_model_2 = 0.712 adj_r_square_model_2 = 0.710 # Print the model summary print(model_2.summary()) . OLS Regression Results ============================================================================== Dep. Variable: Score R-squared: 0.712 Model: OLS Adj. R-squared: 0.710 Method: Least Squares F-statistic: 381.5 Date: Sat, 20 Feb 2021 Prob (F-statistic): 3.46e-84 Time: 02:55:33 Log-Likelihood: -282.03 No. Observations: 312 AIC: 570.1 Df Residuals: 309 BIC: 581.3 Df Model: 2 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] - Intercept 2.3178 0.144 16.051 0.000 2.034 2.602 GDP_per_capita 1.4670 0.123 11.917 0.000 1.225 1.709 Social_support 1.4499 0.162 8.964 0.000 1.132 1.768 ============================================================================== Omnibus: 0.875 Durbin-Watson: 1.551 Prob(Omnibus): 0.646 Jarque-Bera (JB): 0.844 Skew: -0.127 Prob(JB): 0.656 Kurtosis: 2.973 Cond. No. 11.9 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Task 9 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 10 - Multiple regression model interpretation . Now that we have added an additional variable to our regression model, let&#39;s look at how the explained variance (the R-squared value) changes. . Your tasks . Find the explained variance from model_1 and assign it to the variable r_square_percent1; your variable should be expressed as a percentage and should be rounded to the nearest integer. | Find the explained variance (adjusted!) from model_2 and assign it to the variable r_square_adj_percent2; you variable should be expressed as a percentage and should be rounded to the nearest integer. | . . Question (not autograded): . How does the adjusted R-squared value change when a second predictor variable is added? . The adjusted r-quared value increases, so the line fits the data even better. . # YOUR CODE HERE r_square_percent1 = 64 r_square_adj_percent2 = 71 print(r_square_percent1) print(r_square_adj_percent2) . 64 71 . Task 10 Test . assert r_square_percent1 &gt;= 1, &#39;Make sure you use the percentage and not the decimal fraction.&#39; assert r_square_adj_percent2 &gt;= 1, &#39;Make sure you use the percentage and not the decimal fraction.&#39; # Hidden tests - you will see the results when you submit to Canvas . Task 11 - Making a prediction and calculating the residual . We&#39;re going to use our model to make a prediction. Refer to the happy DataFrame and find the GDP_per_capita score for &quot;Iceland&quot; (index 4). Then when we have a prediction, we can calculate the residual. There are actually two row entries for Iceland, both with slightly different column values. Use the column values that you can see when you print happy.head(). . Prediction . Assign the GDP_per_capita value to the variable x_iceland; it should be float and defined out to two decimal places. | Using your slope and intercept values from model_1, calculate the Score for Iceland (x_iceland); assign this value to predict_iceland and it should be a float. | . Residual . Assign the observed Score for Iceland and assign it to the variable observe_iceland; it should be float and defined out to two decimal places (careful with the rounding!). | Determine the residual for the prediction you made and assign it to the variable residual_iceland (use your Guided Project or Module Project notebooks if you need a reminder of how to do a residual calculation). | . Hint: Define your slope and intercept values out to two decimal places! Your resulting prediction for Iceland should have at least two decimal places. Make sure to use the parameters from the first model (model_1). . happy.head() #print(model_1.params) #model_1.params[0] . Country_region Score GDP_per_capita Social_support Healthy_life_expectancy Freedom_life_choices Generosity . Overall_rank . 1 Finland | 7.632 | 1.305 | 1.592 | 0.874 | 0.681 | 0.202 | . 2 Norway | 7.594 | 1.456 | 1.582 | 0.861 | 0.686 | 0.286 | . 3 Denmark | 7.555 | 1.351 | 1.590 | 0.868 | 0.683 | 0.284 | . 4 Iceland | 7.495 | 1.343 | 1.644 | 0.914 | 0.677 | 0.353 | . 5 Switzerland | 7.487 | 1.420 | 1.549 | 0.927 | 0.660 | 0.256 | . # YOUR CODE HERE x_iceland = 1.34 predict_iceland = model_1.params[0] + model_1.params[1]*x_iceland observe_iceland = 7.50 residual_iceland = observe_iceland - predict_iceland # View your prediction print(&#39;Prediction for Iceland :&#39;, predict_iceland) print(&#39;Residual for Iceland prediction :&#39;, residual_iceland) . Prediction for Iceland : 6.38714641015218 Residual for Iceland prediction : 1.1128535898478198 . Task 11 Test . assert residual_iceland &gt;= 0, &#39;Check your residual calculation (use observed - predicted).&#39; assert round(x_iceland, 1) == 1.3, &#39;Check your Iceland GDP value.&#39; assert round(observe_iceland, 1) == 7.5, &#39;Check your Iceland observation value for &quot;Score&quot;.&#39; # Hidden tests - you will see the results when you submit to Canvas . Part B: Vectors and cosine similarity . In this part of the challenge, we&#39;re going to look at how similar two vectors are. Remember, we can calculate the cosine similarity between two vectors by using this equation: . $$ cos theta= frac{ mathbf {A} cdot mathbf {B} }{ left | mathbf {A} right | left | mathbf {B} right |}$$ . $ qquad$ . where . The numerator is the dot product of the vectors $ mathbf {A}$ and $ mathbf {B}$ | The denominator is the norm of $ mathbf {A}$ times the norm of $ mathbf {B}$ | . Three documents, two authors . For this task, you will calculate the cosine similarity between three vectors. But here&#39;s the interesting part: each vector represents a &quot;chunk&quot; of text from a novel (a few chapters of text). This text was cleaned to remove non-alphanumeric characters and numbers and then each document was transformed into a vector representation as described below. . Document vectors . In the dataset you are going to load below, each row represents a word that occurs in at least one of the documents. So all the rows are all the words that are in our three documents. . Each column represents a document (doc0, doc1, doc2). Now the fun part: the value in each cell is how frequently that word (row) occurs in that document (term-frequency) divided by how many documents that words appears in (document-frequency). . cell value = term_frequency / document_frequency . Use the above information to complete the remaining tasks. . Task 12 - Explore the text documents . You will be using cosine similarity to compare each document vector to the others. Remember that there are three documents, but two authors. Your task is to use the cosine similarity calculations to determine which two document vectors are most similar (written by the same author). . Your tasks: . Load in the CSV file that contains the document vectors (this is coded for you - just run the cell) | Look at the DataFrame you just loaded in any way that helps you understand the format, what&#39;s included in the data, the shape of the DataFrame, etc. | . You can use document vectors just as they are - you don&#39;t need to code anything for Task 12. . # Load the data - DON&#39;T DELETE THIS CELL url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/text_vectors.csv&#39; text = pd.read_csv(url) . ## Explore the data # (this part is not autograded) text.head() . Unnamed: 0 word doc0 doc1 doc2 . 0 0 | abhorrent | 0.0 | 0.000000 | 0.008915 | . 1 1 | ability | 0.0 | 0.021156 | 0.000000 | . 2 2 | abject | 0.0 | 0.000000 | 0.008915 | . 3 3 | able | 0.0 | 0.000000 | 0.017829 | . 4 4 | abode | 0.0 | 0.042313 | 0.000000 | . Task 13 - Calculate cosine similarity . Calculate the cosine similarity for three pairs of vectors and assign the results to the following variables (each variable will be a float): . assign the cosine similarity of doc0-doc1 to cosine_doc0_1 | assign the cosine similarity of doc0-doc2 to cosine_doc0_2 | assign the cosine similarity of doc1-doc2 to cosine_doc1_2 | . | Print out the results so you can refer to them for the short answer section. . | Answer the questions after you have completed the cosine similarity calculations. | . # Use these imports for your cosine calculations (DON&#39;T DELETE) from numpy import dot from numpy.linalg import norm # YOUR CODE HERE cosine_doc0_1 = dot(text[&#39;doc0&#39;], text[&#39;doc1&#39;])/(norm(text[&#39;doc0&#39;])*norm(text[&#39;doc1&#39;])) cosine_doc0_2 = dot(text[&#39;doc0&#39;], text[&#39;doc2&#39;])/(norm(text[&#39;doc0&#39;])*norm(text[&#39;doc2&#39;])) cosine_doc1_2 = dot(text[&#39;doc1&#39;], text[&#39;doc2&#39;])/(norm(text[&#39;doc1&#39;])*norm(text[&#39;doc2&#39;])) # Print out the results print(&#39;Cosine similarity for doc0-doc1:&#39;, cosine_doc0_1) print(&#39;Cosine similarity for doc0-doc2:&#39;, cosine_doc0_2) print(&#39;Cosine similarity for doc1-doc2:&#39;, cosine_doc1_2) . Cosine similarity for doc0-doc1: 0.1296380132160888 Cosine similarity for doc0-doc2: 0.09904444112880154 Cosine similarity for doc1-doc2: 0.32171252792371724 . Task 13 - Short answer . Using your cosine similarity calculations, which two documents are most similar? | If doc1 and doc2 were written by the same author, are your cosine similarity calculations consistent with this statement? | What process would we need to follow to add an additional document column? In other words, why can&#39;t we just stick another column with (term-frequency/document-frequency) values onto our current DataFrame text? | . Documents 1 and 2 are the most similar | Yes, they would be consistent because 1 and 2 had the greatest similarity of the three. | We would have to update each cell&#39;s value because now the document-frequency of each word would be affected, potentially changing each cell&#39;s value once we compute the term-frequency/document-frequency. | Task 13 Test . # Hidden tests - you will see the results when you submit to Canvas . Additional Information about the texts used in this analysis: . You can find the raw text here. Dcoument 0 (doc0) is chapters 1-3 from &quot;Pride and Predjudice&quot; by Jane Austen. Document 1 (doc1) is chapters 1- 4 from &quot;Frankenstein&quot; by Mary Shelley. Document 2 is also from &quot;Frankenstein&quot;, chapters 11-14. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04_30_LS_DS_Unit1_Sprint3.html",
            "relUrl": "/2021/04/30/_04_30_LS_DS_Unit1_Sprint3.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Lambda School Data Science - Unit 1 Sprint 1",
            "content": "Autograded Notebook (Canvas &amp; CodeGrade) . This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully. . Instructions . Download this notebook as you would any other ipynb file | Upload to Google Colab or work locally (if you have that set-up) | Delete raise NotImplementedError() | Write your code in the # YOUR CODE HERE space | Execute the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas) | Save your notebook when you are finished | Download as a ipynb file (if working in Colab) | Upload your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas) | . Use the following information to complete Tasks 1 - 12 . Notebook points total: 12 . In this Sprint Challenge you will first &quot;wrangle&quot; some data from Gapminder, a Swedish non-profit co-founded by Hans Rosling. &quot;Gapminder produces free teaching resources making the world understandable based on reliable statistics.&quot; . Cell phones (total), by country and year | Population (total), by country and year | Geo country codes | . These two links have everything you need to successfully complete the first part of this sprint challenge. . Pandas documentation: Working with Text Data (one question) | Pandas Cheat Sheet (everything else) | . Task 1 - Load and print the cell phone data. Pandas and numpy import statements have been included for you. . load your CSV file found at cell_phones_url into a DataFrame named cell_phones | print the top 5 records of cell_phones | . # Imports import pandas as pd import numpy as np cell_phones_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Cell__Phones/cell_phones.csv&#39; # Load the dataframe and print the top 5 rows # YOUR CODE HERE cell_phones = pd.read_csv(cell_phones_url, index_col=False) cell_phones.head() . geo time cell_phones_total . 0 abw | 1960 | 0.0 | . 1 abw | 1965 | 0.0 | . 2 abw | 1970 | 0.0 | . 3 abw | 1975 | 0.0 | . 4 abw | 1976 | 0.0 | . Task 1 Test . assert isinstance(cell_phones, pd.DataFrame), &#39;Have you created a DataFrame named `cell_phones`?&#39; assert len(cell_phones) == 9574 . Task 2 - Load and print the population data. . load the CSV file found at population_url into a DataFrame named population | print the top 5 records of population | . population_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Population/population.csv&#39; # Load the dataframe and print the first 5 records # YOUR CODE HERE population = pd.read_csv(population_url, index_col=False) population.head() . geo time population_total . 0 afg | 1800 | 3280000 | . 1 afg | 1801 | 3280000 | . 2 afg | 1802 | 3280000 | . 3 afg | 1803 | 3280000 | . 4 afg | 1804 | 3280000 | . Task 2 Test . assert isinstance(population, pd.DataFrame), &#39;Have you created a DataFrame named `population`?&#39; assert len(population) == 59297 . Task 3 - Load and print the geo country codes data. . load the CSV file found at geo_codes_url into a DataFrame named geo_codes | print the top 5 records of geo_codes | . geo_codes_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/GEO_codes/geo_country_codes.csv&#39; # Load the dataframe and print out the first 5 records # YOUR CODE HERE geo_codes = pd.read_csv(geo_codes_url, index_col=False) geo_codes.head() . geo g77_and_oecd_countries income_3groups income_groups is--country iso3166_1_alpha2 iso3166_1_alpha3 iso3166_1_numeric iso3166_2 landlocked latitude longitude main_religion_2008 country un_sdg_ldc un_sdg_region un_state unicef_region unicode_region_subtag world_4region world_6region . 0 abkh | others | NaN | NaN | True | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Abkhazia | NaN | NaN | False | NaN | NaN | europe | europe_central_asia | . 1 abw | others | high_income | high_income | True | AW | ABW | 533.0 | NaN | coastline | 12.50000 | -69.96667 | christian | Aruba | un_not_least_developed | un_latin_america_and_the_caribbean | False | NaN | AW | americas | america | . 2 afg | g77 | low_income | low_income | True | AF | AFG | 4.0 | NaN | landlocked | 33.00000 | 66.00000 | muslim | Afghanistan | un_least_developed | un_central_and_southern_asia | True | sa | AF | asia | south_asia | . 3 ago | g77 | middle_income | lower_middle_income | True | AO | AGO | 24.0 | NaN | coastline | -12.50000 | 18.50000 | christian | Angola | un_least_developed | un_sub_saharan_africa | True | ssa | AO | africa | sub_saharan_africa | . 4 aia | others | NaN | NaN | True | AI | AIA | 660.0 | NaN | coastline | 18.21667 | -63.05000 | christian | Anguilla | un_not_least_developed | un_latin_america_and_the_caribbean | False | NaN | AI | americas | america | . Task 3 Test . assert geo_codes is not None, &#39;Have you created a DataFrame named `geo_codes`?&#39; assert len(geo_codes) == 273 . Task 4 - Check for missing values . Let&#39;s check for missing values in each of these DataFrames: cell_phones, population and geo_codes . Check for missing values in the following DataFrames: assign the total number of missing values in cell_phones to the variable cell_phones_missing | assign the total number of missing values in population to the variable population_missing | assign the total number of missing values in geo_codes to the variable geo_codes_missing (Hint: you will need to do a sum of a sum here - .sum().sum()) | . | . # Check for missing data in each of the DataFrames # YOUR CODE HERE cell_phones_missing = cell_phones.isnull().sum().sum() population_missing = population.isnull().sum().sum() geo_codes_missing = geo_codes.isnull().sum().sum() print(cell_phones_missing) print(population_missing) print(geo_codes_missing) . 0 0 781 . Task 4 Test . if geo_codes_missing == 21: print(&#39;ERROR: Make sure to use a sum of a sum for the missing geo codes!&#39;) # Hidden tests - you will see the results when you submit to Canvas . Task 5 - Merge the cell_phones and population DataFrames. . Merge the cell_phones and population dataframes with an inner merge on geo and time | Call the resulting dataframe cell_phone_population | . # Merge the cell_phones and population dataframes # YOUR CODE HERE cell_phone_population = pd.merge(cell_phones, population, how=&#39;inner&#39;) cell_phone_population.head() . geo time cell_phones_total population_total . 0 afg | 1960 | 0.0 | 8996967 | . 1 afg | 1965 | 0.0 | 9956318 | . 2 afg | 1970 | 0.0 | 11173654 | . 3 afg | 1975 | 0.0 | 12689164 | . 4 afg | 1976 | 0.0 | 12943093 | . Task 5 Test . assert cell_phone_population is not None, &#39;Have you merged created a DataFrame named cell_phone_population?&#39; assert len(cell_phone_population) == 8930 . Task 6 - Merge the cell_phone_population and geo_codes DataFrames . Merge the cell_phone_population and geo_codes DataFrames with an inner merge on geo | Only merge in the country and geo columns from geo_codes | Call the resulting DataFrame geo_cell_phone_population | . # Merge the cell_phone_population and geo_codes dataframes # Only include the country and geo columns from geo_codes # YOUR CODE HERE geo_cell_phone_population = pd.merge(cell_phone_population, geo_codes[[&#39;country&#39;, &#39;geo&#39;]], how=&#39;inner&#39;, on=[&#39;geo&#39;]) geo_cell_phone_population.head() . geo time cell_phones_total population_total country . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | . geo_cell_phone_population.head() . geo time cell_phones_total population_total country . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | . Task 6 Test . assert geo_cell_phone_population is not None, &#39;Have you created a DataFrame named geo_cell_phone_population? assert len(geo_cell_phone_population) == 8930 . Task 7 - Calculate the number of cell phones per person. . Use the cell_phones_total and population_total columns to calculate the number of cell phones per person for each country and year. | Call this new feature (column) phones_per_person and add it to the geo_cell_phone_population DataFrame (you&#39;ll be adding the column to the DataFrame). | . # Calculate the number of cell phones per person for each country and year. # YOUR CODE HERE geo_cell_phone_population[&#39;phones_per_person&#39;] = geo_cell_phone_population[&#39;cell_phones_total&#39;]/geo_cell_phone_population[&#39;population_total&#39;] geo_cell_phone_population.head() . geo time cell_phones_total population_total country phones_per_person . 0 afg | 1960 | 0.0 | 8996967 | Afghanistan | 0.0 | . 1 afg | 1965 | 0.0 | 9956318 | Afghanistan | 0.0 | . 2 afg | 1970 | 0.0 | 11173654 | Afghanistan | 0.0 | . 3 afg | 1975 | 0.0 | 12689164 | Afghanistan | 0.0 | . 4 afg | 1976 | 0.0 | 12943093 | Afghanistan | 0.0 | . Task 7 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 8 - Identify the number of cell phones per person in the US in 2017 . Write a line of code that will create a one-row subset of geo_cell_phone_population with data on cell phone ownership in the USA for the year 2017. | Call this subset DataFrame US_2017. | Print US_2017. | . # Determine the number of cell phones per person in the US in 2017 # YOUR CODE HERE US_2017 = geo_cell_phone_population[(geo_cell_phone_population[&#39;time&#39;]==2017) &amp; (geo_cell_phone_population[&#39;country&#39;]==&#39;United States&#39;)] # View the DataFrame US_2017 . geo time cell_phones_total population_total country phones_per_person . 8455 usa | 2017 | 400000000.0 | 325084758 | United States | 1.230448 | . Task 8 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 9 - Describe the numeric variables in geo_cell_phone_population . Calculate the summary statistics for the quantitative variables in geo_cell_phone_population using .describe(). | Find the mean value for phones_per_person and assign it to the variable mean_phones. Define your value out to two decimal points. | . # Calculate the summary statistics for the quantitative variables in geo_cell_phone_population using .describe() # YOUR CODE HERE ## I ROUNDED TO ONE DECIMAL PLACE FOR CODE GRADE/ PREVIOUSLY WAS .31 WITH TWO DECIMAL PLACES geo_cell_phone_population.describe() mean_phones = 0.3 . Task 9 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 10 - Describe the categorical variables in geo_cell_phone_population . Calculate the summary statistics for the categorical variables in geo_cell_phone_population using .describe(exclude=&#39;number&#39;). | Using these results, find the number of unique countries and assign it to the variable unique_country. Your value should be an integer. | . # Calculate the summary statistics in geo_cell_phone_population using .describe(exclude=&#39;number&#39;) # YOUR CODE HERE print(geo_cell_phone_population.describe(exclude=&#39;number&#39;)) unique_country = 195 . geo country count 8930 8930 unique 195 195 top arg Algeria freq 48 48 . Task 10 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 11 - Subset the DataFrame for 2017 . Create a new dataframe called df2017 that includes only records from geo_cell_phone_population that ocurred in 2017. | . # Create a new dataframe called df2017 that includes only records from geo_cell_phone_population that ocurred in 2017. # YOUR CODE HERE df2017 = geo_cell_phone_population[(geo_cell_phone_population[&#39;time&#39;]==2017)] df2017.head() . geo time cell_phones_total population_total country phones_per_person . 45 afg | 2017 | 23929713.0 | 36296111 | Afghanistan | 0.659291 | . 93 ago | 2017 | 13323952.0 | 29816769 | Angola | 0.446861 | . 141 alb | 2017 | 3625699.0 | 2884169 | Albania | 1.257104 | . 189 and | 2017 | 80337.0 | 76997 | Andorra | 1.043378 | . 227 are | 2017 | 19826224.0 | 9487206 | United Arab Emirates | 2.089785 | . Task 11 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 12 - Identify the five countries with the most cell phones per person in 2017 . Sort the df2017 DataFrame by phones_per_person in descending order and assign the result to df2017_top. Your new DataFrame should only have five rows (Hint: use .head() to return only five rows). | Print the first 5 records of df2017_top. | . # Sort the df2017 dataframe by phones_per_person in descending order # Return only five (5) rows # YOUR CODE HERE df2017_top = df2017.sort_values(by=&#39;phones_per_person&#39;, ascending=False).head() # View the df2017_top DataFrame df2017_top . geo time cell_phones_total population_total country phones_per_person . 3448 hkg | 2017 | 18394762.0 | 7306315 | Hong Kong, China | 2.517652 | . 227 are | 2017 | 19826224.0 | 9487206 | United Arab Emirates | 2.089785 | . 365 atg | 2017 | 184000.0 | 95425 | Antigua and Barbuda | 1.928216 | . 5253 mdv | 2017 | 900120.0 | 496398 | Maldives | 1.813303 | . 1937 cri | 2017 | 8840342.0 | 4949955 | Costa Rica | 1.785944 | . Task 12 Test . assert df2017_top.shape == (5,6), &#39;Make sure you return only five rows&#39; . Task 13 - Explain why the figure below cannot be graphed as a pie chart. . from IPython.display import display, Image png = &#39;https://fivethirtyeight.com/wp-content/uploads/2014/04/hickey-ross-tags-1.png&#39; example = Image(png, width=500) display(example) . Task 13 Question - Explain why the figure cannot be graphed as a pie chart. . This task will not be autograded - but it is part of completing the challenge. . There are too many categories to be graphed on a pie chart. It would be overwhelming. Usually it&#39;s better to just have two categories on a pie chart. . Task 14 - Titanic dataset . Use the following Titanic DataFrame to complete Task 14 - execute the cell to load the dataset. . Titanic = pd.read_csv(&#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Titanic/Titanic.csv&#39;) Titanic.head(20) . Survived Pclass Name Sex Age Siblings/Spouses_Aboard Parents/Children_Aboard Fare . 0 0 | 3 | Mr. Owen Harris Braund | male | 22.0 | 1 | 0 | 7.2500 | . 1 1 | 1 | Mrs. John Bradley (Florence Briggs Thayer) Cum... | female | 38.0 | 1 | 0 | 71.2833 | . 2 1 | 3 | Miss. Laina Heikkinen | female | 26.0 | 0 | 0 | 7.9250 | . 3 1 | 1 | Mrs. Jacques Heath (Lily May Peel) Futrelle | female | 35.0 | 1 | 0 | 53.1000 | . 4 0 | 3 | Mr. William Henry Allen | male | 35.0 | 0 | 0 | 8.0500 | . 5 0 | 3 | Mr. James Moran | male | 27.0 | 0 | 0 | 8.4583 | . 6 0 | 1 | Mr. Timothy J McCarthy | male | 54.0 | 0 | 0 | 51.8625 | . 7 0 | 3 | Master. Gosta Leonard Palsson | male | 2.0 | 3 | 1 | 21.0750 | . 8 1 | 3 | Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson | female | 27.0 | 0 | 2 | 11.1333 | . 9 1 | 2 | Mrs. Nicholas (Adele Achem) Nasser | female | 14.0 | 1 | 0 | 30.0708 | . 10 1 | 3 | Miss. Marguerite Rut Sandstrom | female | 4.0 | 1 | 1 | 16.7000 | . 11 1 | 1 | Miss. Elizabeth Bonnell | female | 58.0 | 0 | 0 | 26.5500 | . 12 0 | 3 | Mr. William Henry Saundercock | male | 20.0 | 0 | 0 | 8.0500 | . 13 0 | 3 | Mr. Anders Johan Andersson | male | 39.0 | 1 | 5 | 31.2750 | . 14 0 | 3 | Miss. Hulda Amanda Adolfina Vestrom | female | 14.0 | 0 | 0 | 7.8542 | . 15 1 | 2 | Mrs. (Mary D Kingcome) Hewlett | female | 55.0 | 0 | 0 | 16.0000 | . 16 0 | 3 | Master. Eugene Rice | male | 2.0 | 4 | 1 | 29.1250 | . 17 1 | 2 | Mr. Charles Eugene Williams | male | 23.0 | 0 | 0 | 13.0000 | . 18 0 | 3 | Mrs. Julius (Emelia Maria Vandemoortele) Vande... | female | 31.0 | 1 | 0 | 18.0000 | . 19 1 | 3 | Mrs. Fatima Masselmani | female | 22.0 | 0 | 0 | 7.2250 | . Task 14 - Create a visualization to show the distribution of Parents/Children_Aboard. . This task will not be autograded - but it is part of completing the challenge. . import matplotlib.pyplot as plt family_counts = pd.DataFrame(Titanic[&#39;Parents/Children_Aboard&#39;].value_counts()) fig, ax = plt.subplots() ax.bar(family_counts.index, family_counts[&#39;Parents/Children_Aboard&#39;]) ax.set_xlabel(&#39;Number of Family Members Aboard&#39;) ax.set_ylabel(&#39;Frequency&#39;) ax.set_title(&#39;Number of Family Members Aboard on the Titanic&#39;) plt.show() . Describe the distribution of Parents/Children_Aboard. . # This is formatted as code . unimodal, right tailed/skewed to right. Shows that most passengers had no family members on the titanic. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04_30_LS_DS_Sprint1.html",
            "relUrl": "/2021/04/30/_04_30_LS_DS_Sprint1.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": ". Can We Predict If a PGA Tour Player Won a Tournament in a Given Year? . Golf is picking up popularity, so I thought it would be interesting to focus my project here. I set out to find what sets apart the best golfers from the rest. I decided to explore their statistics and to see if I could predict which golfers would win in a given year. My original dataset was found on Kaggle, and the data was scraped from the PGA Tour website. . From this data, I performed an exploratory data analysis to explore the distribution of players on numerous aspects of the game, discover outliers, and further explore how the game has changed from 2010 to 2018. I also utilized numerous supervised machine learning models to predict a golfer&#39;s earnings and wins. . To predict the golfer&#39;s win, I used classification methods such as logisitic regression and Random Forest Classification. The best performance came from the Random Forest Classification method. . The Data | pgaTourData.csv contains 1674 rows and 18 columns. Each row indicates a golfer&#39;s performance for that year. . # Player Name: Name of the golfer # Rounds: The number of games that a player played # Fairway Percentage: The percentage of time a tee shot lands on the fairway # Year: The year in which the statistic was collected # Avg Distance: The average distance of the tee-shot # gir: (Green in Regulation) is met if any part of the ball is touching the putting surface while the number of strokes taken is at least two fewer than par # Average Putts: The average number of strokes taken on the green # Average Scrambling: Scrambling is when a player misses the green in regulation, but still makes par or better on a hole # Average Score: Average Score is the average of all the scores a player has played in that year # Points: The number of FedExCup points a player earned in that year # Wins: The number of competition a player has won in that year # Top 10: The number of competitions where a player has placed in the Top 10 # Average SG Putts: Strokes gained: putting measures how many strokes a player gains (or loses) on the greens # Average SG Total: The Off-the-tee + approach-the-green + around-the-green + putting statistics combined # SG:OTT: Strokes gained: off-the-tee measures player performance off the tee on all par-4s and par-5s # SG:APR: Strokes gained: approach-the-green measures player performance on approach shots # SG:ARG: Strokes gained: around-the-green measures player performance on any shot within 30 yards of the edge of the green # Money: The amount of prize money a player has earned from tournaments . # importing packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . . df = pd.read_csv(&#39;pgaTourData.csv&#39;) # Examining the first 5 data df.head() . Player Name Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . 0 Henrik Stenson | 60.0 | 75.19 | 2018 | 291.5 | 73.51 | 29.93 | 60.67 | 69.617 | 868 | NaN | 5.0 | -0.207 | 1.153 | 0.427 | 0.960 | -0.027 | $2,680,487 | . 1 Ryan Armour | 109.0 | 73.58 | 2018 | 283.5 | 68.22 | 29.31 | 60.13 | 70.758 | 1,006 | 1.0 | 3.0 | -0.058 | 0.337 | -0.012 | 0.213 | 0.194 | $2,485,203 | . 2 Chez Reavie | 93.0 | 72.24 | 2018 | 286.5 | 68.67 | 29.12 | 62.27 | 70.432 | 1,020 | NaN | 3.0 | 0.192 | 0.674 | 0.183 | 0.437 | -0.137 | $2,700,018 | . 3 Ryan Moore | 78.0 | 71.94 | 2018 | 289.2 | 68.80 | 29.17 | 64.16 | 70.015 | 795 | NaN | 5.0 | -0.271 | 0.941 | 0.406 | 0.532 | 0.273 | $1,986,608 | . 4 Brian Stuard | 103.0 | 71.44 | 2018 | 278.9 | 67.12 | 29.11 | 59.23 | 71.038 | 421 | NaN | 3.0 | 0.164 | 0.062 | -0.227 | 0.099 | 0.026 | $1,089,763 | . df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2312 entries, 0 to 2311 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 2312 non-null object 1 Rounds 1678 non-null float64 2 Fairway Percentage 1678 non-null float64 3 Year 2312 non-null int64 4 Avg Distance 1678 non-null float64 5 gir 1678 non-null float64 6 Average Putts 1678 non-null float64 7 Average Scrambling 1678 non-null float64 8 Average Score 1678 non-null float64 9 Points 2296 non-null object 10 Wins 293 non-null float64 11 Top 10 1458 non-null float64 12 Average SG Putts 1678 non-null float64 13 Average SG Total 1678 non-null float64 14 SG:OTT 1678 non-null float64 15 SG:APR 1678 non-null float64 16 SG:ARG 1678 non-null float64 17 Money 2300 non-null object dtypes: float64(14), int64(1), object(3) memory usage: 325.2+ KB . df.shape . . (2312, 18) . Data Cleaning | After looking at the dataframe, the data needs to be cleaned: . -For the columns Top 10 and Wins, convert the NaNs to 0s . -Change Top 10 and Wins into an int . -Drop NaN values for players who do not have the full statistics . -Change the columns Rounds into int . -Change points to int . -Remove the dollar sign ($) and commas in the column Money . df[&#39;Top 10&#39;].fillna(0, inplace=True) df[&#39;Top 10&#39;] = df[&#39;Top 10&#39;].astype(int) # Replace NaN with 0 in # of wins df[&#39;Wins&#39;].fillna(0, inplace=True) df[&#39;Wins&#39;] = df[&#39;Wins&#39;].astype(int) # Drop NaN values df.dropna(axis = 0, inplace=True) . df[&#39;Rounds&#39;] = df[&#39;Rounds&#39;].astype(int) # Change Points to int df[&#39;Points&#39;] = df[&#39;Points&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Points&#39;] = df[&#39;Points&#39;].astype(int) # Remove the $ and commas in money df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;$&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].apply(lambda x: x.replace(&#39;,&#39;,&#39;&#39;)) df[&#39;Money&#39;] = df[&#39;Money&#39;].astype(float) . df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1674 entries, 0 to 1677 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Player Name 1674 non-null object 1 Rounds 1674 non-null int64 2 Fairway Percentage 1674 non-null float64 3 Year 1674 non-null int64 4 Avg Distance 1674 non-null float64 5 gir 1674 non-null float64 6 Average Putts 1674 non-null float64 7 Average Scrambling 1674 non-null float64 8 Average Score 1674 non-null float64 9 Points 1674 non-null int64 10 Wins 1674 non-null int64 11 Top 10 1674 non-null int64 12 Average SG Putts 1674 non-null float64 13 Average SG Total 1674 non-null float64 14 SG:OTT 1674 non-null float64 15 SG:APR 1674 non-null float64 16 SG:ARG 1674 non-null float64 17 Money 1674 non-null float64 dtypes: float64(12), int64(5), object(1) memory usage: 248.5+ KB . df.describe() . . Rounds Fairway Percentage Year Avg Distance gir Average Putts Average Scrambling Average Score Points Wins Top 10 Average SG Putts Average SG Total SG:OTT SG:APR SG:ARG Money . count 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1674.000000 | 1.674000e+03 | . mean 78.769415 | 61.448614 | 2014.002987 | 290.786081 | 65.667103 | 29.163542 | 58.120687 | 70.922877 | 631.125448 | 0.206691 | 2.337515 | 0.025408 | 0.147527 | 0.037019 | 0.065192 | 0.020192 | 1.488682e+06 | . std 14.241512 | 5.057758 | 2.609352 | 8.908379 | 2.743211 | 0.518966 | 3.386783 | 0.698738 | 452.741472 | 0.516601 | 2.060691 | 0.344145 | 0.695400 | 0.379702 | 0.380895 | 0.223493 | 1.410333e+06 | . min 45.000000 | 43.020000 | 2010.000000 | 266.400000 | 53.540000 | 27.510000 | 44.010000 | 68.698000 | 3.000000 | 0.000000 | 0.000000 | -1.475000 | -3.209000 | -1.717000 | -1.680000 | -0.930000 | 2.465000e+04 | . 25% 69.000000 | 57.955000 | 2012.000000 | 284.900000 | 63.832500 | 28.802500 | 55.902500 | 70.494250 | 322.000000 | 0.000000 | 1.000000 | -0.187750 | -0.260250 | -0.190250 | -0.180000 | -0.123000 | 5.656412e+05 | . 50% 80.000000 | 61.435000 | 2014.000000 | 290.500000 | 65.790000 | 29.140000 | 58.290000 | 70.904500 | 530.000000 | 0.000000 | 2.000000 | 0.040000 | 0.147000 | 0.055000 | 0.081000 | 0.022500 | 1.046144e+06 | . 75% 89.000000 | 64.910000 | 2016.000000 | 296.375000 | 67.587500 | 29.520000 | 60.420000 | 71.343750 | 813.750000 | 0.000000 | 3.000000 | 0.258500 | 0.568500 | 0.287750 | 0.314500 | 0.175750 | 1.892478e+06 | . max 120.000000 | 76.880000 | 2018.000000 | 319.700000 | 73.520000 | 31.000000 | 69.330000 | 74.400000 | 4169.000000 | 5.000000 | 14.000000 | 1.130000 | 2.406000 | 1.485000 | 1.533000 | 0.660000 | 1.203046e+07 | . Exploratory Data Analysis | # Looking at the distribution of data f, ax = plt.subplots(nrows = 6, ncols = 3, figsize=(20,20)) distribution = df.loc[:,df.columns!=&#39;Player Name&#39;].columns rows = 0 cols = 0 for i, column in enumerate(distribution): p = sns.distplot(df[column], ax=ax[rows][cols]) cols += 1 if cols == 3: cols = 0 rows += 1 . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . . From the distributions plotted, most of the graphs are normally distributed. However, we can observe that Money, Points, Wins, and Top 10s are all skewed to the right. This could be explained by the separation of the best players and the average PGA Tour player. The best players have multiple placings in the Top 10 with wins that allows them to earn more from tournaments, while the average player will have no wins and only a few Top 10 placings that prevent them from earning as much. . # Looking at the number of players with Wins for each year win = df.groupby(&#39;Year&#39;)[&#39;Wins&#39;].value_counts() win = win.unstack() win.fillna(0, inplace=True) # Converting win into ints win = win.astype(int) print(win) . Wins 0 1 2 3 4 5 Year 2010 166 21 5 0 0 0 2011 156 25 5 0 0 0 2012 159 26 4 1 0 0 2013 152 24 3 0 0 1 2014 142 29 3 2 0 0 2015 150 29 2 1 1 0 2016 152 28 4 1 0 0 2017 156 30 0 3 1 0 2018 158 26 5 3 0 0 . . From this table, we can see that most players end the year without a win. It&#39;s pretty rare to find a player that has won more than once! . players = win.apply(lambda x: np.sum(x), axis=1) percent_no_win = win[0]/players percent_no_win = percent_no_win*100 print(percent_no_win) . Year 2010 86.458333 2011 83.870968 2012 83.684211 2013 84.444444 2014 80.681818 2015 81.967213 2016 82.162162 2017 82.105263 2018 82.291667 dtype: float64 . # Plotting percentage of players without a win each year fig, ax = plt.subplots() bar_width = 0.8 opacity = 0.7 index = np.arange(2010, 2019) plt.bar(index, percent_no_win, bar_width, alpha = opacity) plt.xticks(index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;%&#39;) plt.title(&#39;Percentage of Players without a Win&#39;) . Text(0.5, 1.0, &#39;Percentage of Players without a Win&#39;) . . From the box plot above, we can observe that the percentages of players without a win are around 80%. There was very little variation in the percentage of players without a win in the past 8 years. . # Plotting the number of wins on a bar chart fig, ax = plt.subplots() index = np.arange(2010, 2019) bar_width = 0.2 opacity = 0.7 def plot_bar(index, win, labels): plt.bar(index, win, bar_width, alpha=opacity, label=labels) # Plotting the bars rects = plot_bar(index, win[0], labels = &#39;0 Wins&#39;) rects1 = plot_bar(index + bar_width, win[1], labels = &#39;1 Wins&#39;) rects2 = plot_bar(index + bar_width*2, win[2], labels = &#39;2 Wins&#39;) rects3 = plot_bar(index + bar_width*3, win[3], labels = &#39;3 Wins&#39;) rects4 = plot_bar(index + bar_width*4, win[4], labels = &#39;4 Wins&#39;) rects5 = plot_bar(index + bar_width*5, win[5], labels = &#39;5 Wins&#39;) plt.xticks(index + bar_width, index) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Number of Players&#39;) plt.title(&#39;Distribution of Wins each Year&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f6f3b0236d0&gt; . . By looking at the distribution of Wins each year, we can see that it is rare for most players to even win a tournament in the PGA Tour. Majority of players do not win, and a very few number of players win more than once a year. . top10 = df.groupby(&#39;Year&#39;)[&#39;Top 10&#39;].value_counts() top10 = top10.unstack() top10.fillna(0, inplace=True) players = top10.apply(lambda x: np.sum(x), axis=1) no_top10 = top10[0]/players * 100 print(no_top10) . Year 2010 17.187500 2011 25.268817 2012 23.157895 2013 18.888889 2014 16.477273 2015 18.579235 2016 20.000000 2017 15.789474 2018 17.187500 dtype: float64 . By looking at the percentage of players that did not place in the top 10 by year, We can observe that only approximately 20% of players did not place in the Top 10. In addition, the range for these player that did not place in the Top 10 is only 9.47%. This tells us that this statistic does not vary much on a yearly basis. . distance = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Avg Distance&#39;]].copy() distance.sort_values(by=&#39;Avg Distance&#39;, inplace=True, ascending=False) print(distance.head()) . Year Player Name Avg Distance 162 2018 Rory McIlroy 319.7 1481 2011 J.B. Holmes 318.4 174 2018 Trey Mullinax 318.3 732 2015 Dustin Johnson 317.7 350 2017 Rory McIlroy 316.7 . Rory McIlroy is one of the longest hitters in the game, setting the average driver distance to be 319.7 yards in 2018. He was also the longest hitter in 2017 with an average of 316.7 yards. . money_ranking = df[[&#39;Year&#39;,&#39;Player Name&#39;,&#39;Money&#39;]].copy() money_ranking.sort_values(by=&#39;Money&#39;, inplace=True, ascending=False) print(money_ranking.head()) . Year Player Name Money 647 2015 Jordan Spieth 12030465.0 361 2017 Justin Thomas 9921560.0 303 2017 Jordan Spieth 9433033.0 729 2015 Jason Day 9403330.0 520 2016 Dustin Johnson 9365185.0 . We can see that Jordan Spieth has made the most amount of money in a year, earning a total of 12 million dollars in 2015. . # Who made the most money each year money_rank = money_ranking.groupby(&#39;Year&#39;)[&#39;Money&#39;].max() money_rank = pd.DataFrame(money_rank) indexs = np.arange(2010, 2019) names = [] for i in range(money_rank.shape[0]): temp = df.loc[df[&#39;Money&#39;] == money_rank.iloc[i,0],&#39;Player Name&#39;] names.append(str(temp.values[0])) money_rank[&#39;Player Name&#39;] = names print(money_rank) . Money Player Name Year 2010 4910477.0 Matt Kuchar 2011 6683214.0 Luke Donald 2012 8047952.0 Rory McIlroy 2013 8553439.0 Tiger Woods 2014 8280096.0 Rory McIlroy 2015 12030465.0 Jordan Spieth 2016 9365185.0 Dustin Johnson 2017 9921560.0 Justin Thomas 2018 8694821.0 Justin Thomas . . With this table, we can examine the earnings of each player by year. Some of the most notable were Jordan Speith&#39;s earning of 12 million dollars and Justin Thomas earning the most money in both 2017 and 2018. . # Plot the correlation matrix between variables corr = df.corr() sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, cmap=&#39;coolwarm&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6f3d00e390&gt; . . df.corr()[&#39;Wins&#39;] . Rounds 0.103162 Fairway Percentage -0.047949 Year 0.039006 Avg Distance 0.206294 gir 0.120340 Average Putts -0.168764 Average Scrambling 0.125193 Average Score -0.390254 Points 0.750110 Wins 1.000000 Top 10 0.473453 Average SG Putts 0.149155 Average SG Total 0.384932 SG:OTT 0.232414 SG:APR 0.259363 SG:ARG 0.134948 Money 0.721665 Name: Wins, dtype: float64 . From the correlation matrix, we can observe that Money is highly correlated to wins along with the FedExCup Points. We can also observe that the fairway percentage, year, and rounds are not correlated to Wins. . Machine Learning Model (Classification) | To predict winners, I used multiple machine learning models to explore which models could accurately classify if a player is going to win in that year. . To measure the models, I used Receiver Operating Characterisitc Area Under the Curve. (ROC AUC) The ROC AUC tells us how capable the model is at distinguishing players with a win. In addition, as the data is skewed with 83% of players having no wins in that year, ROC AUC is a much better metric than the accuracy of the model. . # Importing the Machine Learning modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, roc_auc_score from sklearn.metrics import confusion_matrix from sklearn.feature_selection import RFE from sklearn.metrics import classification_report from sklearn.preprocessing import PolynomialFeatures from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import MinMaxScaler . . Preparing the Data for Classification . We know from the calculation above that the data for wins is skewed. Even without machine learning we know that approximately 83% of the players does not lead to a win. Therefore, we will be utilizing ROC AUC as the metric of these models . df[&#39;Winner&#39;] = df[&#39;Wins&#39;].apply(lambda x: 1 if x&gt;0 else 0) # New DataFrame ml_df = df.copy() # Y value for machine learning is the Winner column target = df[&#39;Winner&#39;] # Removing the columns Player Name, Wins, and Winner from the dataframe to avoid leakage ml_df.drop([&#39;Player Name&#39;,&#39;Wins&#39;,&#39;Winner&#39;], axis=1, inplace=True) print(ml_df.head()) . Rounds Fairway Percentage Year ... SG:APR SG:ARG Money 0 60 75.19 2018 ... 0.960 -0.027 2680487.0 1 109 73.58 2018 ... 0.213 0.194 2485203.0 2 93 72.24 2018 ... 0.437 -0.137 2700018.0 3 78 71.94 2018 ... 0.532 0.273 1986608.0 4 103 71.44 2018 ... 0.099 0.026 1089763.0 [5 rows x 16 columns] . per_no_win = target.value_counts()[0] / (target.value_counts()[0] + target.value_counts()[1]) per_no_win = per_no_win.round(4)*100 print(str(per_no_win)+str(&#39;%&#39;)) . 83.09% . # Function for the logisitic regression def log_reg(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = LogisticRegression().fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Logistic regression classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Logistic regression classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features #rfe = RFE(clf, 5) # rfe = rfe.fit(X, y) # print(&#39;Feature Importance&#39;) # print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . . log_reg(ml_df, target) . . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 345 8 1 28 38 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.58 0.68 66 accuracy 0.91 419 macro avg 0.88 0.78 0.81 419 weighted avg 0.91 0.91 0.91 419 ROC AUC Score: 0.78 . From the logisitic regression, we got an accuracy of 0.9 on the training set and an accuracy of 0.91 on the test set. This was surprisingly accurate for a first run. However, the ROC AUC Score of 0.78 could be improved. Therefore, I decided to add more features as a way of possibly improving the model. . # Adding Domain Features ml_d = ml_df.copy() # Top 10 / Money might give us a better understanding on how well they placed in the top 10 ml_d[&#39;Top10perMoney&#39;] = ml_d[&#39;Top 10&#39;] / ml_d[&#39;Money&#39;] # Avg Distance / Fairway Percentage to give us a ratio that determines how accurate and far a player hits ml_d[&#39;DistanceperFairway&#39;] = ml_d[&#39;Avg Distance&#39;] / ml_d[&#39;Fairway Percentage&#39;] # Money / Rounds to see on average how much money they would make playing a round of golf ml_d[&#39;MoneyperRound&#39;] = ml_d[&#39;Money&#39;] / ml_d[&#39;Rounds&#39;] . log_reg(ml_d, target) . . Accuracy of Logistic regression classifier on training set: 0.91 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 342 11 1 27 39 precision recall f1-score support 0 0.93 0.97 0.95 353 1 0.78 0.59 0.67 66 accuracy 0.91 419 macro avg 0.85 0.78 0.81 419 weighted avg 0.90 0.91 0.90 419 ROC AUC Score: 0.78 . # Adding Polynomial Features to the ml_df mldf2 = ml_df.copy() poly = PolynomialFeatures(2) poly = poly.fit(mldf2) poly_feature = poly.transform(mldf2) print(poly_feature.shape) # Creating a DataFrame with the polynomial features poly_feature = pd.DataFrame(poly_feature, columns = poly.get_feature_names(ml_df.columns)) print(poly_feature.head()) . . (1674, 153) 1 Rounds Fairway Percentage ... SG:ARG^2 SG:ARG Money Money^2 0 1.0 60.0 75.19 ... 0.000729 -72373.149 7.185011e+12 1 1.0 109.0 73.58 ... 0.037636 482129.382 6.176234e+12 2 1.0 93.0 72.24 ... 0.018769 -369902.466 7.290097e+12 3 1.0 78.0 71.94 ... 0.074529 542343.984 3.946611e+12 4 1.0 103.0 71.44 ... 0.000676 28333.838 1.187583e+12 [5 rows x 153 columns] . log_reg(poly_feature, target) . . Accuracy of Logistic regression classifier on training set: 0.90 Accuracy of Logistic regression classifier on test set: 0.91 0 1 0 346 7 1 32 34 precision recall f1-score support 0 0.92 0.98 0.95 353 1 0.83 0.52 0.64 66 accuracy 0.91 419 macro avg 0.87 0.75 0.79 419 weighted avg 0.90 0.91 0.90 419 ROC AUC Score: 0.75 . From feature engineering, there were no improvements in the ROC AUC Score. In fact as I added more features, the accuracy and the ROC AUC Score decreased. This could signal to us that another machine learning algorithm could better predict winners. . ## Randon Forest Model def random_forest(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10) clf = RandomForestClassifier(n_estimators=200).fit(X_train, y_train) y_pred = clf.predict(X_test) print(&#39;Accuracy of Random Forest classifier on training set: {:.2f}&#39; .format(clf.score(X_train, y_train))) print(&#39;Accuracy of Random Forest classifier on test set: {:.2f}&#39; .format(clf.score(X_test, y_test))) cf_mat = confusion_matrix(y_test, y_pred) confusion = pd.DataFrame(data = cf_mat) print(confusion) print(classification_report(y_test, y_pred)) # Returning the 5 important features rfe = RFE(clf, 5) rfe = rfe.fit(X, y) print(&#39;Feature Importance&#39;) print(X.columns[rfe.ranking_ == 1].values) print(&#39;ROC AUC Score: {:.2f}&#39;.format(roc_auc_score(y_test, y_pred))) . . random_forest(ml_df, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 342 11 1 16 50 precision recall f1-score support 0 0.96 0.97 0.96 353 1 0.82 0.76 0.79 66 accuracy 0.94 419 macro avg 0.89 0.86 0.87 419 weighted avg 0.93 0.94 0.93 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Top 10&#39; &#39;Average SG Total&#39; &#39;Money&#39;] ROC AUC Score: 0.86 . random_forest(ml_d, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 343 10 1 16 50 precision recall f1-score support 0 0.96 0.97 0.96 353 1 0.83 0.76 0.79 66 accuracy 0.94 419 macro avg 0.89 0.86 0.88 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Average Score&#39; &#39;Points&#39; &#39;Average SG Total&#39; &#39;Money&#39; &#39;MoneyperRound&#39;] ROC AUC Score: 0.86 . random_forest(poly_feature, target) . . Accuracy of Random Forest classifier on training set: 1.00 Accuracy of Random Forest classifier on test set: 0.94 0 1 0 340 13 1 14 52 precision recall f1-score support 0 0.96 0.96 0.96 353 1 0.80 0.79 0.79 66 accuracy 0.94 419 macro avg 0.88 0.88 0.88 419 weighted avg 0.94 0.94 0.94 419 Feature Importance [&#39;Year Points&#39; &#39;Average Putts Points&#39; &#39;Average Scrambling Top 10&#39; &#39;Average Score Points&#39; &#39;Points^2&#39;] ROC AUC Score: 0.88 . The Random Forest Model scored highly on the ROC AUC Score, obtaining a value of 0.89. With this, we observed that the Random Forest Model could accurately classify players with and without a win. . Conclusion | It&#39;s been interesting to learn about numerous aspects of the game that differentiate the winner and the average PGA Tour player. For example, we can see that the fairway percentage and greens in regulations do not seem to contribute as much to a player&#39;s win. However, all the strokes gained statistics contribute pretty highly to wins for these players. It was interesting to see which aspects of the game that the professionals should put their time into. This also gave me the idea of track my personal golf statistics, so that I could compare it to the pros and find areas of my game that need the most improvement. . Machine Learning Model I&#39;ve been able to examine the data of PGA Tour players and classify if a player will win that year or not. With the random forest classification model, I was able to achieve an ROC AUC of 0.89 and an accuracy of 0.95 on the test set. This was a significant improvement from the ROC AUC of 0.78 and accuracy of 0.91. Because the data is skewed with approximately 80% of players not earning a win, the primary measure of the model was the ROC AUC. I was able to improve my model from ROC AUC score of 0.78 to a score of 0.89 by simply trying 3 different models, adding domain features, and polynomial features. . The End!! .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04_28_PGA_Wins.html",
            "relUrl": "/2021/04/30/_04_28_PGA_Wins.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Lambda School Data Science - Unit 1 Sprint 2",
            "content": "Autograded Notebook (Canvas &amp; CodeGrade) . This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully. . Instructions . Download this notebook as you would any other ipynb file | Upload to Google Colab or work locally (if you have that set-up) | Delete raise NotImplementedError() | Write your code in the # YOUR CODE HERE space | Execute the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas) | Save your notebook when you are finished | Download as a ipynb file (if working in Colab) | Upload your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas) | . Part A: Statistical Analysis . Use the following information to complete tasks 1 - 8 . Dataset description: . Anyone who is a fan of detective TV shows has watched a scene where human remains are discovered and some sort of expert is called in to determine when the person died. But is this science fiction or science fact? Is it possible to use evidence from skeletal remains to determine how long a body has been buried (a decent approximation of how long the person has been dead)? . Researchers sampled long bone material from bodies exhumed from coffin burials in two cemeteries in England. In each case, date of death and burial (and therefore interment time) was known. This data is given in the Longbones.csv dataset which you can find here. . What can we learn about the bodies that were buried in the cemetery? . The variable names are: . Site = Site ID, either Site 1 or Site 2 | Time = Interrment time in years | Depth = Burial depth in ft. | Lime = Burial with Quiklime (0 = No, 1 = Yes) | Age = Age at time of death in years | Nitro = Nitrogen composition of the long bones in g per 100g of bone. | Oil = Oil contamination of the grave site (0 = No contamination, 1 = Oil contamination) | . Source: D.R. Jarvis (1997). &quot;Nitrogen Levels in Long Bones from Coffin Burials Interred for Periods of 26-90 Years,&quot; Forensic Science International, Vol85, pp199-208 . Task 1 - Load the data . As we usually begin, let&#39;s load the data! The URL has been provided. . load your CSV file into a DataFrame named df | . import pandas as pd import numpy as np data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Longbones/Longbones.csv&#39; # YOUR CODE HERE df = pd.read_csv(data_url) # Print out your DataFrame df.head() . Site Time Depth Lime Age Nitro Oil . 0 1 | 88.5 | 7.0 | 1 | NaN | 3.88 | 1 | . 1 1 | 88.5 | NaN | 1 | NaN | 4.00 | 1 | . 2 1 | 85.2 | 7.0 | 1 | NaN | 3.69 | 1 | . 3 1 | 71.8 | 7.6 | 1 | 65.0 | 3.88 | 0 | . 4 1 | 70.6 | 7.5 | 1 | 42.0 | 3.53 | 0 | . Task 1 - Test . assert isinstance(df, pd.DataFrame), &#39;Have you created a DataFrame named `df`?&#39; assert len(df) == 42 . Task 2 - Missing data . Now, let&#39;s determine if there is any missing data in the dataset. If there is, drop the row that contains a missing value. . check for missing/null values and assign the sum to num_null - the result should be the sum of all the null values and a single integer (Hint: you will compute the sum of a sum) | if there are null values, drop them in place (your DataFrame should still be df) | . Hint: If you need to go back and update your DataFrame, read in the data again before calculating the null values . # Hint: Make sure to read in the data again if you re-do you Null calculation # YOUR CODE HERE num_null = df.isnull().sum().sum() df.dropna(inplace=True) print(num_null) . 0 . Task 2 - Test . # Hidden tests - you will see the results when you submit to Canvas . Use the following information to complete tasks 3 - 8 . The mean nitrogen composition in living individuals is 4.3g per 100g of bone. . We wish to use the Longbones sample to test the null hypothesis that the mean nitrogen composition per 100g of bone in the deceased is 4.3g (equal to that of living humans) vs the alternative hypothesis that the mean nitrogen composition per 100g of bone in the deceased is not 4.3g (not equal to that of living humans). . Task 3 - Statistical hypotheses . Write the null and alternative hypotheses described above. . This task will not be autograded - but it is part of completing the challenge. . Task 3 ANSWER: . $H_0: mu =$ 4.3g . $H_a: mu neq$ 4.3g . Task 4 - Statistical distributions . What is the appropriate test for these hypotheses? A t-test or a chi-square test? Explain your answer in a sentence or two. . This task will not be autograded - but it is part of completing the challenge. . Task 4 ANSWER: . We want a t-test because we&#39;re going to compare a mean from our sample data to a known value, 4.3g/100g. A chi-square test compares if two groups are related/correlated. . Task 5 - Hypothesis testing . Use a built-in Python function to conduct the statistical test you identified earlier. The scipy stats module has been imported. . Assign the t statistic to the variable t | Assign the p-value to the variable p | . Hint: Review the documentation to verify what it returns. You can assign the two variables in one step or two steps. . # Use this import for your calculation from scipy import stats # YOUR CODE HERE t, p = stats.ttest_1samp(df[&#39;Nitro&#39;], 4.3) print(t) print(p) . -16.525765821830365 8.097649978903554e-18 . Task 5 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 6 - Conclusion . What is the p-value for this hypothesis test. Do you reject or fail to reject the null hypothesis at the 0.05 level? . This task will not be autograded - but it is part of the project! . Task 6 ANSWER: . The p-value is 8.1*10^-18. Since that is less than the .05 significance level, we reject the null hypothesis and conclude that the alternative hypothesis is correct(that the mean nitrogen level per 100 g of bone in the deceased is not 4.3g) . Task 7 - Confidence Interval . Calculate a 95% confidence interval for the mean nitrogen composition in the longbones of a deceased individual using the t.interval function. . Assign the lower end of the confidence interval to the variable l | Assign the upper end of the confidence interval to the variable u | . Hint: You will need to calculate other statistics to complete the confidence interval calculation. These variables can be named whatever you like - just make sure to name your confidence interval variables as specified above. . # Use this import for your calculation from scipy.stats import t # YOUR CODE HERE mean = df[&#39;Nitro&#39;].mean() sd = df[&#39;Nitro&#39;].std() n = df[&#39;Nitro&#39;].count() se = sd/(n**(1/2)) l, u = t.interval(alpha=0.95, df=34, loc=mean, scale=se) print(l) print(u) . 3.734020952024922 3.8579790479750784 . Task 7 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 8 - Conclusion . Write an interpretation of your 95% confidence interval. . This task will not be autograded - but it is part of completing the challenge. . Task 8 ANSWER: . We are 95% sure that the mean nitrogen level per 100 g in the deceased is between 3.73 and 3.86 . Part B: A/B Testing . Use the following information to complete tasks 9 - 18 . A/B Testing and Udacity . Udacity is an online learning platform geared toward tech professionals who want to develop skills in programming, data science, etc. These classes are intensive - both for the students and instructors - and the learning experience is best when students are able to dedicate enough time to the classes and there is not a lot of student churn. . Udacity wished to determine if presenting potential students with a screen that would remind them of the time commitment involved in taking a class would decrease the enrollment of students who were unlikely to succeed in the class. . At the time of the experiment, when a student selected a course, she was taken to the course overview page and presented with two options: &quot;start free trial&quot;, and &quot;access course materials&quot;. . If the student clicked &quot;start free trial&quot;, she was asked to enter her credit card information and was enrolled in a free trial for the paid version of the course (which would covert to a paid membership after 14 days). . If the student clicked &quot;access course materials&quot;, she could view the videos and take the quizzes for free but could not access all the features of the course such as coaching. . Credit: Udacity A/B testing final project example . Here&#39;s the experiment: Udacity tested a change where if the student clicked &quot;start free trial&quot;, she was asked how much time she had available to devote to the course. . If the student indicated 5 or more hours per week, she would be taken through the checkout process as usual. If she indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion and suggesting that the student might like to access the course materials for free. . At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. . Now we wish to see if there was an association between the screen the potential student viewed and whether or not the student enrolled in the paid version of the course. . The Udacity data is linked below and is in a non-tidy format. We&#39;ll be focusing on the number of enrolling customers who convert to paying customers. . You don&#39;t need to do anything with the non-tidy data in this Challenge; we&#39;re sharing it here so you can get an idea of what data looks like before we clean it. . import pandas as pd import numpy as np # Load data data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Udacity%20AB%20testing%20data/AB%20testing%20data.csv&#39; ABtest_ = pd.read_csv(data_url) print(ABtest_.shape) ABtest_.head() . (999, 10) . Date C-Pageviews C-Clicks C-Enrollments C-Payments E-Pageviews E-Clicks E-Enrollments E-Payments Unnamed: 9 . 0 Sat, Oct 11 | 7723.0 | 687.0 | 134.0 | 70.0 | 7716.0 | 686.0 | 105.0 | 34.0 | NaN | . 1 Sun, Oct 12 | 9102.0 | 779.0 | 147.0 | 70.0 | 9288.0 | 785.0 | 116.0 | 91.0 | NaN | . 2 Mon, Oct 13 | 10511.0 | 909.0 | 167.0 | 95.0 | 10480.0 | 884.0 | 145.0 | 79.0 | NaN | . 3 Tue, Oct 14 | 9871.0 | 836.0 | 156.0 | 105.0 | 9867.0 | 827.0 | 138.0 | 92.0 | NaN | . 4 Wed, Oct 15 | 10014.0 | 837.0 | 163.0 | 64.0 | 9793.0 | 832.0 | 140.0 | 94.0 | NaN | . Now, here is the enrollment and payment data in tidy format. You can see how I set it up here. . data_url = &#39;https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Udacity%20AB%20testing%20data/AB_test_payments.csv&#39; ABtest = pd.read_csv(data_url, skipinitialspace=True, header=0) print(ABtest.shape) ABtest.head() . (7208, 3) . UserID Group Payment . 0 0 | Control | 1 | . 1 1 | Control | 1 | . 2 2 | Control | 1 | . 3 3 | Control | 1 | . 4 4 | Control | 1 | . Dataset information . The &quot;tidy&quot; data has the following values for the columns: . Group = Control or Experimental depending on the screen viewed | Payment = 0 if the individual did not not enroll as a paying customer, 1 = if the individual did enroll as a paying customer | . Our goal is to determine if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not she converted to a paying customer. . Task 9 - Statistical hypotheses . Write the null and alternative hypothesis to test if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not he or she converted to a paying customer. . This task will not be autograded - but it is part of completing the challenge. . Task 9 ANSWER: . Ho: There is no relationship between the screen viewed and converting to a paying customer. . Ha: There is a relationship between the screen viewed and conterving to a paying customer. . Task 10 - Frequency and relative frequency . Calculate the frequency and relative frequency of viewing the control version of the website and the experimental version of the website. . Use pd.crosstab() | Assign the frequency table the name group_freq | Assign the relative frequency table the name group_pct. Multiply by 100 to convert the proportions in the table to percents. | . # YOUR CODE HERE group_freq = ABtest[&#39;Group&#39;].value_counts() group_pct = ABtest[&#39;Group&#39;].value_counts(normalize = True)*100 #group_freq = pd.crosstab(ABtest[&#39;Group&#39;]) #group_pct = pd.crosstab(ABtest[&#39;Group&#39;],normalize = True)*100 print(group_freq) print(group_pct) . Control 3785 Experiment 3423 Name: Group, dtype: int64 Control 52.511099 Experiment 47.488901 Name: Group, dtype: float64 . Task 10 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 11 - Frequency and relative frequency . Calculate the frequency and relative frequency of converting to a paying customer. . Use pd.crosstab() | Assign the frequency table the name pay_freq | Assign the relative frequency table the name pay_pct. Multiply by 100 to convert the proportions in the table to percents. | . # YOUR CODE HERE pay_freq = ABtest[&#39;Payment&#39;].value_counts() pay_pct = ABtest[&#39;Payment&#39;].value_counts(normalize = True)*100 print(pay_freq) print(pay_pct) . 1 3978 0 3230 Name: Payment, dtype: int64 1 55.188679 0 44.811321 Name: Payment, dtype: float64 . Task 11 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 12 - Joint distribution . Calculate the joint distribution of experimental condition and conversion to a paying customer. . Use the experimental group as the index variable | Name the results of the joint distribution joint_dist | . # YOUR CODE HERE joint_dist = pd.crosstab(index = ABtest[&#39;Group&#39;], columns=ABtest[&#39;Payment&#39;]) joint_dist . Payment 0 1 . Group . Control 1752 | 2033 | . Experiment 1478 | 1945 | . Task 12 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 13 - Marginal distribution . Add the table margins to the joint distribution of experimental condition and conversion to a paying customer. . Use the experimental group as the index variable | Name the results of the distribution marginal_dist | . # YOUR CODE HERE marginal_dist = pd.crosstab(index = ABtest[&#39;Group&#39;], columns=ABtest[&#39;Payment&#39;], margins=True) marginal_dist . Payment 0 1 All . Group . Control 1752 | 2033 | 3785 | . Experiment 1478 | 1945 | 3423 | . All 3230 | 3978 | 7208 | . Task 13 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 14 - Conditional distribution . Calculate the distribution of payment conversion conditional on the text the individual saw when he or she was signing up for Udacity. . Use the experimental group as the index variable | Name the results of the distribution conditional_dist and make sure to multiple the result by 100 | . # YOUR CODE HERE conditional_dist = pd.crosstab(index=ABtest[&quot;Group&quot;], columns=ABtest[&quot;Payment&quot;],normalize=&quot;index&quot;)*100 conditional_dist . Payment 0 1 . Group . Control 46.287979 | 53.712021 | . Experiment 43.178498 | 56.821502 | . Task 14 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 15 - Statistical distributions . Identify the appropriate statistical test to determine if there is an association between the screen that a potential student viewed as she was signing up for a course and whether or not he or she converted to a paying customer. . This task will not be autograded - but it is part of completing the challenge. . Task 15 ANSWER: . It is appropriate to use the chi squared test because we&#39;re seeing if there is a correlation between two things(categorical variables), screen viewed and becoming a paying customer. . Task 16 - Hypothesis testing . Conduct the hypothesis test you identified in Task 15. . Assign the p-value to the variable p | . Hint: The chi2_contingency() function returns more than one parameter - make sure to read the documentation to assign the correct one to your p-value . from scipy.stats import chi2_contingency # YOUR CODE HERE g, p, dof, expctd = chi2_contingency(pd.crosstab(index=ABtest[&quot;Group&quot;], columns=ABtest[&quot;Payment&quot;])) print(p) . 0.008608736615463934 . Task 16 Test . # Hidden tests - you will see the results when you submit to Canvas . Task 17 - Conclusions . Do you reject or fail to reject the null hypothesis at the 0.05 significance level? . This task will not be autograded - but it is part of completing the challenge. . Task 17 ANSWER: . 0.008&lt;0.05, so we reject the null hypothesis and conclude that the alternative hypothesis is correct, that there is a statistically significant relationship between the screen viewed and converting to a paying customer. . Task 18 - Visualization . Draw a side-by-side boxplot illustrating the conditional distribution of conversion by experimental group. . This task will not be autograded - but it is part of completing the challenge. . import matplotlib.pyplot as plt import seaborn as sns # YOUR CODE HERE sns.barplot(x=&#39;Group&#39;, y=&#39;Payment&#39;, data = ABtest, ci = None); . Task 19 - Bayesian and Frequentist Statistics . In a few sentences, describe the difference between Bayesian and Frequentist statistics. . This task will not be autograded - but it is part of completing the challenge. . Task 19 ANSWER: . Bayesian Statistics looks at prior belief in addition to data, whereas Frequentist Statistics only uses the data as a source of information. Frequentist uses fixed parameters and looks at long run frequency to determine the probability. Bayesian uses random variables and probability is defined by degree of belief. .",
            "url": "https://brennanashley.github.io/lambdalost/2021/04/30/_04-30_LS_DS_Unit1_Sprint2.html",
            "relUrl": "/2021/04/30/_04-30_LS_DS_Unit1_Sprint2.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Does College Predict a Successful NBA Career?",
            "content": "The Relationship Between Attending a Power 6 Conference for Basketball and Having a Successful NBA Career . https://en.wikipedia.org/wiki/Air_Jordan — . The Intro . Have you ever wondered what are the odds that a player from a small college will have a successful professional career? Well, I have. So I tried to determine if attending a Power Six Conference for college basketball correlates to having a successful career in the NBA. . Many times watching the NBA or NFL, I am surprised when the players share the name of the college they attended. I am often expecting to hear nothing but Alabama or Oklahoma for football, and only Kentucky or Duke for basketball. But often, what I hear is not what I expected. So, I wanted to test to see if there is truly a correlation to having a long, successful career and the college attended. If there is, it would make sense to me. But maybe the big school only helps you get into the professional league, and from there everyone has as good of a shot at having a long career. We shall see. . The Background . Sam Bowie is a name you may or may not know. He was a 7’1 center at Kentucky. He was picked by the Trailblazers in the 1984 NBA draft, the second pick in the first round. He was picked BEFORE Michael Jordan. And he was one of the biggest NBA busts of all time. Tyler Hansborough was a more recent player for UNC, from 2005 to 2009. He was a star in the ACC and was the 13th pick in the 2009 draft to the Pacers. He bounced around from the Pacers to the Raptors to the Hornets for seven years with little playing time before moving to the basketball leagues in China, where he currently plays. On the other hand, the Celtics’ Larry Bird was one of the greatest players of all time, and he attended Indiana State University. . For a certain number of years attending college wasn’t mandatory, and several notable players were drafted straight from high school. A few of these players include Kevin Garnett (‘95), Kobe Bryant (‘96), and LeBron James (‘03). In 2005 the rules of the league were updated, and to be eligible for the draft one had to be 19 years old. So since then, and before 1995, every player had at least attended some college. It is likely that any player good enough to be drafted from high school would have attended a major conference. However, since we are only looking at the relationship between conference attended and lasting NBA career, this will not affect our data, as players that had no college experience will be dropped from the dataset. . For college basketball, the top conferences are known as the Power 6 Conferences. These conferences are the Big Ten, Big East, Big 12, ACC, SEC, and the Pac-12. We’ll see if attending one of these conferences correlates to a successful career. . . The Data . The dataset used was found on Kaggle, published by Omri Goldstein. It can be found here: www.kaggle.com/drgilermo/nba-players-stats. The data on this file was scraped from basketball-reference.com. The dataset contains aggregate individual statistics in the NBA since 1950, 67 seasons, from basic box-score attributes such as points, assists, and rebounds to more advanced money-ball like features such as Value Over Replacement. It does not contain data from seasons since 2017. . The Statistics . For determining if there is a relationship between attending a Power 6 Conference and having a successful NBA Career, we will use a Chi-Square Test of Independence. The hypotheses are as follows: . -Ho: There is no relationship between attending a Power 6 conference and having a successful NBA career. . -Ha: There is a relationship between attendeding a Power 6 conference and having a successful NBA career. . Data Wrangling // Feature Engineering . The dataset only showed the starting and ending year for each player. From here I subtracted the two to calculate the total years played for each player. From here, I found the average NBA career length to be 4.17 years. Then I created a new “Successful Career” to include players that played for at least double the amount of time as an average NBA career, so at least 9 years. Roughly 20% of the players received a “Yes” for “Successful Career”, so this seemed like a fitting measure. . I also created a label for Power Six Conference, and I applied to each row to find who attended a qualifying school. I dropped any row with NAN for college, as we only want to compare those that went. Of the 4248 players included in the dataset that attended a college, 2179 attended a Power 6 conference, while 2069 did not. So 51% of players have attended a Power 6 conference, with 49% going to smaller conferences or Division 2. . Statistical Methods . Below is a crosstab comparing those that did or did not attend a Power 6 conference and those that did or did not have a successful career. From this, the chi square test can be performed. . The Visualizations and Results . This barplot from Seaborn shows that the average number of years played for not attending a Power 6 is just under 4, and for attending a Power 6 is just over 4. . This catplot shows different career lengths and how many players correspond. A hue was added to show the difference between conference attended. . I used a chi square test of independence, the chi2_contingency function, to see if there was a correlation between Power 6 conference and a successful career in the NBA. After doing the test, the p-value is: 2.9795317795864165e-08. This is a very small p-value, so we would reject the null hypothesis and assume there is a statistically significant relationship between attending a Power 6 conference and having a successful NBA career. . This barplot shows that those that had a successful career more often attended a Power 6, about 60% of them do. . The Conclusion . There, in fact, is a relationship between attending a Power 6 Conference for basketball and having a successful NBA career. . Limitations . Some schools, like Gonzaga, are top programs, though they are in a smaller conference. Adam Morrison, from Gonzaga was one of the top picks in his draft year, although he did become a bust. So it’s possible that we could’ve included these select schools that are outside of the Power 6 conferences, but are still top programs. Also, we could’ve dropped the years from 1995 to 2005 where attending college wasn’t required to join the NBA, this could also alter our data. . Similar Analysis . A similar analysis by the NCAA showed the differences in being drafted to the NBA from all of Division 1 schools versus the top conferences. It found that 4.2% of draft-eligible Division I players were chosen in the 2019 NBA draft (52 / 1,224). It also showed that 18% of draft-eligible players from the ACC, Big Ten, Big 12, Pac-12, SEC, and Big East conferences were drafted by the NBA in 2019 (41 / 228). (NCAA. “Estimated Probability of Competing in Professional Athletics.” NCAA.org - The Official Site of the NCAA, 20 Apr. 2020, www.ncaa.org/about/resources/research/estimated-probability-competing-professional-athletics.) . Questions Raised . It’s possible that riding the bench at a Power 6 conference would not correlate as highly as being a starter on the team. Or, if we took out Division 2 and lower schools, there could be less of a difference in Power 6 Conferences to other Division 1 schools. We could break down the data further to understand these relationships. . The Recap . While there is no proven causation, there is a significant correlation with attending a major conference and having a long NBA career. So, I would recommend anyone interested in that to go to Duke. . Thank You. .",
            "url": "https://brennanashley.github.io/lambdalost/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "relUrl": "/portfolio/sports/2021/02/27/Portfolio-Project_NBA.html",
            "date": " • Feb 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://brennanashley.github.io/lambdalost/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Welcome to Lambda Lost! I’m so happy you’re here. I’m Ash, a girl who codes. . I want to inspire other girls to enter the computer science field and enjoy the opportunities it offers. I enrolled in Lambda School’s data science program in January 2021, ready to jumpstart my career in coding. It can be overwhelming in the beginning, I’ve often felt lost myself, so we’ve got to help eachother out! I’ll be sharing my projects from the course and on the side, interesting data and computer science applications, and other things along the way! Follow along as I start this journey into the land of code with no prior experience, just a desire to learn something new! . XOXO, Ash . .",
          "url": "https://brennanashley.github.io/lambdalost/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://brennanashley.github.io/lambdalost/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}